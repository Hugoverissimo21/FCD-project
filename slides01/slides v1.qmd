---
title: "titulo"
subtitle: "subtitulo?"
author: 
  - Hugo Veríssimo 124348
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: ua.png
    css: mystyle.css
    theme: serif
    transition: slide
echo: true
---

```{r setup, include = FALSE}
# packages
library(dplyr)
library(knitr)
library(xtable)
library(reticulate)
```

```{python, include=FALSE}
import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime
```



# Context and Objective

```{=html}
<style>
.dataframe {
  display: block;
  max-width: 100%;
  max-height: 75%; /* vertical scrolling */
  overflow-x: auto;
  overflow-y: auto;
  font-family: "SFMono-Regular", Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
  /*border: 1px solid #ddd; apply only to big dfs */
}

table {
  max-width: 100%;
  border-collapse: collapse;
}

th, td { /* th is about header*/
  padding: 8px 16px;
  border: 1px solid #ddd; /* Border between cells */
  text-align: left;
  vertical-align: middle;
  font-size: 16px;
}

thead th {
  background-color: rgba(128, 128, 128, 0.3);
  font-weight: bold;
}

tbody td:first-child {
  background-color: rgba(128, 128, 128, 0.3);
  font-weight: bold;
}
</style>
```

## . {.justify}

- intro qq

- o meu objetivo

- trabalhos parecidos

# Data Extraction from arquivo.pt

## companies {.justify}

first i had to select which companies i would be analyzing and set their aliases (names that they are known for) from psi20 selected 5 more or less randomly

```{python}
companies = {"Banco Comercial Português": ["Banco Comercial Português", "BCP"],
             "Galp Energia": ["Galp Energia", "GALP"],
             "EDP": ["EDP", "Energias de Portugal", "Electricidade de Portugal"],
             "Sonae": ["Sonae", "SON"],
             "Mota-Engil": ["Mota-Engil", "EGL"]}
```




## News Sources Selection {.justify}    

```{.python}
pd.read_csv('noticias.csv')
```

```{python, echo=FALSE}
pd.read_csv('noticias.csv', header=None)
```

```{python}
def news(csvFile = 'noticias.csv'):
    """
    grab the news websites from a csv file
    """
    links = pd.read_csv(csvFile, header=None).iloc[:,0]
    return ",".join(links)
```

## API Request Function

```{.python code-line-numbers="1-8|9-20|21-25"}
def api_request(search, websites, date):
    """
    search: expression/word (what to look for)
    websites: comma separated websites (where to look for)
    date: list such as [20030101, 20031231] (when to look for)
    -
    returns the responde_items from arquivo.pt api
    """
    search = f"q=%22{search.replace(' ', '%20')}%22"
    websites = f"&siteSearch={websites}"
    date = f"&from={date[0]}&to={date[1]}"    
    url = (
        f"https://arquivo.pt/textsearch?{search}{websites}{date}"
        "&fields=linkToArchive,linkToExtractedText,tstamp"
        "&maxItems=500"
        "&dedupValue=25"
        "&dedupField=url"
        "&prettyPrint=false"
        "&type=html"
        )
    json = requests.get(url).json()
    data = json["response_items"]
    if len(data) == 500:
        print(f"You might have lost some data: {search, date}")
    return data
```

## API Request

grupos de 3 para n ter mtas colunas e para todos as colunas terem dados

torna mais facil analisar à mao celulas para encontrar problemas

```{.python code-line-numbers="1-8|9-20|21-40"} 
# POR FAZER
def datav1(companies):
    """
    Select companies for extraction and save the data into a Parquet file.
    
    - companies (dict): A dictionary of companies with their aliases.
    - companies_Ex = {"Galp Energia": ["Galp Energia", "GALP"],
                      "EDP": ["EDP", "Energias de Portugal"]}
    """
    # CREATING DF WITH COMPANIES AND THEIR ALIASES
    companies_data = {"companies": [], "aliases": []}
    for company in companies.keys():
        companies_data["companies"].append(company)
        companies_data["aliases"].append(companies[company])
    df = pd.DataFrame(companies_data).set_index("companies")

    # SITES OF WHERE TO LOOK FOR NEWS
    websites = news()

    # INITIALIZAING API REQUESTS
    # groups of 3 years, from 2000 to 2020
    for cluster in range(2000, 2021, 3):
        api_cluster = [] #reset api_cluster for each cluster (group of 3 year)
        print(f"Processing cluster: {cluster}")
        print("Processing company:", end=" ")
        # iterate over each company
        for company_aliases in df["aliases"]:
            api_company = [] #reset api_company for each company
            print(f"{company_aliases[0]}", end = "; ")
            # iterate over each company's aliases
            for alias in company_aliases:
                # iterate over each cluter's year
                for year in range(cluster, cluster + 3):                        
                    api_aliasS1 = api_request(alias, websites, [int(f"{year}0101"), int(f"{year}0630")])
                    api_aliasS2 = api_request(alias, websites, [int(f"{year}0701"), int(f"{year}1231")])
                    api_company += api_aliasS1 + api_aliasS2
            # save company data
            api_cluster.append(api_company)

        # save cluster (group of 3 years) data
        df[f"api.{cluster}"] = api_cluster
        print(f"{cluster} OK.")

    # save all data
    df.to_parquet("data01.parquet")
    print("Finished.")
```


## data01.parquet

```{=html}
<table style="border: 1px solid #ddd;" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>aliases</th>
<th>api.2000</th>
<th>api.2003</th>
<th>api.2006</th>
<th>api.2009</th>
<th>api.2012</th>
<th>api.2015</th>
<th>api.2018</th>
</tr>
<tr>
<th>companies</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>Banco Comercial Português</th>
<td>[Banco Comercial Português, BCP]</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
</tr>
<tr>
<th>Galp Energia</th>
<td>[Galp Energia, GALP]</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
</tr>
<tr>
<th>EDP</th>
<td>[EDP, Energias de Portu...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
</tr>
<tr>
<th>Sonae</th>
<td>[Sonae, SON]</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
</tr>
<tr>
<th>Mota-Engil</th>
<td>[Mota-Engil, EGL]</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
</tr>
</tbody>
</table>
```

## a

```{.python}
pd.read_parquet("data01.parquet").map(len)
```

```{=html}
<table class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>aliases</th>
<th>api.2000</th>
<th>api.2003</th>
<th>api.2006</th>
<th>api.2009</th>
<th>api.2012</th>
<th>api.2015</th>
<th>api.2018</th>
</tr>
<tr>
<th>companies</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>Banco Comercial Português</th>
<td style="text-align: right">2</td>
<td style="text-align: right">153</td>
<td style="text-align: right">241</td>
<td style="text-align: right">183</td>
<td style="text-align: right">561</td>
<td style="text-align: right">1074</td>
<td style="text-align: right">1430</td>
<td style="text-align: right">954</td>
</tr>
<tr>
<th>Galp Energia</th>
<td>2</td>
<td>128</td>
<td>389</td>
<td>272</td>
<td>582</td>
<td>1156</td>
<td>1391</td>
<td>968</td>
</tr>
<tr>
<th>EDP</th>
<td>3</td>
<td>133</td>
<td>339</td>
<td>173</td>
<td>653</td>
<td>1232</td>
<td>1970</td>
<td>1096</td>
</tr>
<tr>
<th>Sonae</th>
<td>2</td>
<td>192</td>
<td>435</td>
<td>279</td>
<td>502</td>
<td>1215</td>
<td>1705</td>
<td>1196</td>
</tr>
<tr>
<th>Mota-Engil</th>
<td>2</td>
<td>4</td>
<td>83</td>
<td>60</td>
<td>195</td>
<td>538</td>
<td>828</td>
<td>560</td>
</tr>
</tbody>
</table>
```



```{python}
#pd.read_parquet("data01.parquet")["api.2000"].loc["Mota-Engil"]
pd.read_parquet("data01.parquet").iloc[0,1][0]
```



# text extratcion


## problems

por ter usado `&dedupValue=25&dedupField=url` e diferentes aliases, há informação repetida

**filtrar repetidos e textos que não mencionem nenhum alias**

problemas ultrapassados:

- API has the following usage limits (250req/min, error 429): `time.sleep(60)`

- API error 404 for some urls: return 0 (False) and skip it

- extrair o texto demora muito: filtrar e salvar coluna a coluna

nota: podia ter feito já online o processamento do texto, mas não queria estar dependente do wifi


## extract text function

```{python}
def extracText(linkToExtractedText):
    # Infinite loop to handle retry logic in case of 429 Too Many Requests
    while True:
        response = requests.get(linkToExtractedText)
        status_code = response.status_code
        
        if status_code == 200:
            # If the request is successful (200 OK), return the extracted text
            soup = BeautifulSoup(response.content, "html.parser")
            return soup.get_text()
        elif status_code == 429:
            # Handle 429 Too Many Requests by reading the Retry-After header
            print(" (...)", end = "")
            time.sleep(60)  # Pause execution for the retry period
        elif status_code == 404:
            return 0
        else:
            # For any other status codes (e.g., 500, ...), print the status and break the loop
            print(f"Request failed: {status_code}; Link was {linkToExtractedText}")
            break
```

## applying it

because it takes a long to time to extract all the text (around 2h for me) i made a function to apply the extraticon to each column at a time so i could stop any minute or if i got an error dont need to do everything or if i only wanted to process a col to see how it comes out

```{python}
# Function to process each column
def filterColumn(column, aliases):
    """aliases in text, repeated text and extract text"""
    filtered_column = []

    for row in aliases.index:
        filtered_cell = []
        seen_text = set()
        print(f"; {row}", end = "")
        for i in column.loc[row]:
            
            # Extract text from 'linkToExtractedText'
            text = extracText(i['linkToExtractedText'])
            
            # Skip if the text has already been processed
            if text in seen_text:
                continue

            elif not text: #ERROR 404
                continue
            
            # Check if any alias is found in the text
            elif any(alias.lower() in text.lower() for alias in aliases.loc[row]):
                i["ExtractedText"] = text  # Add extracted text to the record
                
                # Remove unwanted fields
                i.pop('linkToExtractedText', None)
                
                # Append the processed record
                filtered_cell.append(i)
                
                # Mark this text as processed
                seen_text.add(text)

        filtered_column.append(filtered_cell)
                
    return filtered_column
```

## applying it pt2

select which cols to process and skip the ones already done

```{python}
def processColumns(col_to_proc):
    print(f"Starting: {datetime.now()}")
    try:
        # continuar df criada
        df = pd.read_parquet("data02.parquet")
    except:
       # criar df para trabalhar
       df = pd.read_parquet("data01.parquet").to_parquet("data02.parquet")
       df = pd.read_parquet("data02.parquet")
    for column in col_to_proc:
        has_link = "linkToExtractedText" in df.iloc[-1][column][-1]
        has_extracText = "ExtractedText" in df.iloc[-1][column][-1]
        if not has_link and has_extracText:
            print(f"\n{column} already done. Skipping.")
        else:
            print(f"\nProcessing {column}", end = ": ")
            df[column] = filterColumn(df[column], df["aliases"])
            df.to_parquet("data02.parquet")
    print(f"\nEnded: {datetime.now()}.")

#processColumns(["api.2000", "api.2003", "api.2006", "api.2009", "api.2012", "api.2015", "api.2018"])
```



## data02.parquet

the only difference is the lists content (dicitionary) and the qty of news

```{.python}
pd.read_parquet("data02.parquet").map(lambda x: len(x)) - pd.read_parquet("data01.parquet").map(lambda x: len(x))
```

```{=html}
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>aliases</th>
<th>api.2000</th>
<th>api.2003</th>
<th>api.2006</th>
<th>api.2009</th>
<th>api.2012</th>
<th>api.2015</th>
<th>api.2018</th>
</tr>
<tr>
<th>companies</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>Banco Comercial Português</th>
<td style="text-align: right">0</td>
<td style="text-align: right">-63</td>
<td style="text-align: right">-50</td>
<td style="text-align: right">-14</td>
<td style="text-align: right">-64</td>
<td style="text-align: right">-91</td>
<td style="text-align: right">-211</td>
<td style="text-align: right">-130</td>
</tr>
<tr>
<th>Galp Energia</th>
<td>0</td>
<td>-62</td>
<td>-156</td>
<td>-91</td>
<td>-113</td>
<td>-192</td>
<td>-287</td>
<td>-156</td>
</tr>
<tr>
<th>EDP</th>
<td>0</td>
<td>-53</td>
<td>-94</td>
<td>-33</td>
<td>-115</td>
<td>-156</td>
<td>-442</td>
<td>-224</td>
</tr>
<tr>
<th>Sonae</th>
<td>0</td>
<td>-62</td>
<td>-117</td>
<td>-40</td>
<td>-43</td>
<td>-106</td>
<td>-305</td>
<td>-170</td>
</tr>
<tr>
<th>Mota-Engil</th>
<td>0</td>
<td>-1</td>
<td>-16</td>
<td>-34</td>
<td>-31</td>
<td>-154</td>
<td>-232</td>
<td>-115</td>
</tr>
</tbody>
</table>
```

```{.python}
pd.read_parquet("data02.parquet").iloc[0,1][0]
```

```{python, echo=TRUE}
dict = pd.read_parquet("data02.parquet").iloc[0,1][0]
new_dict = {"tstamp": dict["tstamp"],
            "linkToArchive": dict["linkToArchive"],
            "ExtractedText": dict["ExtractedText"]}
            
new_dict # create function to read dictionarys
```


# data filtering


## decision tree

```{=html}
<div style="text-align: center;">
    <img src="dtree01.svg" alt="Description of image" style="width: 100%;">
</div>
```

## percentagens distribution

```{=html}
<div style="text-align: center;">
    <img src="dtree01 (percentages).svg" alt="Description of image" style="width: 100%;">
</div>
```
