---
title: "Media Analysis of PSI-20 Companies"
subtitle: "Insights from News and Public Coverage"
author:
  - name: "Hugo Veríssimo"
    affiliation: "124348"
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: ua.png
    css: mystyle.css
    theme: serif
    transition: slide
echo: true
---

```{r setup, include = FALSE}
# packages
library(dplyr)
library(knitr)
library(xtable)
library(reticulate)
```

```{python, include=FALSE}
import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime
```

## Objective & Methodology {.justify}

```{=html}
<style>
.dataframe {
  display: block;
  max-width: 100%;
  max-height: 75%; /* vertical scrolling */
  overflow-x: auto;
  overflow-y: auto;
  font-family: "SFMono-Regular", Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
  /*border: 1px solid #ddd; apply only to big dfs */
}

table {
  max-width: 100%;
  border-collapse: collapse;
}

th, td { /* th is about header*/
  padding: 8px 16px;
  border: 1px solid #ddd; /* Border between cells */
  text-align: left;
  vertical-align: middle;
  font-size: 16px;
}

thead th {
  background-color: rgba(128, 128, 128, 0.3);
  font-weight: bold;
}

tbody td:first-child {
  background-color: rgba(128, 128, 128, 0.3);
  font-weight: bold;
}
</style>
```

Discover insights about specific companies, traded on the PSI-20 index, by analyzing media mentions, focusing on:

- Sentiment analysis

- Named entity recognition

Similar works:

- [https://github.com/politiquices](https://github.com/politiquices)

- [https://github.com/msramalho/desarquivo](https://github.com/msramalho/desarquivo)

Data Source: [Arquivo.pt web archive](https://arquivo.pt)

# Data Extraction from arquivo.pt

## Companies and News Sources {.justify}

<!-- first i had to select which companies i would be analyzing and set their aliases (names that they are known for) from psi20 selected 5 more or less randomly ;; also i had to select which news sources i would be using -->

```{.python}
companies = {"Banco Comercial Português": ["Banco Comercial Português", "BCP"],
             "Galp Energia": ["Galp Energia", "GALP"],
             "EDP": ["EDP", "Energias de Portugal", "Electricidade de Portugal"],
             "Sonae": ["Sonae", "SON"],
             "Mota-Engil": ["Mota-Engil", "EGL"]}
             
pd.read_csv('noticias.csv')             
```

```{python, echo=FALSE}
pd.read_csv('noticias.csv', header=None)
```

## API Request Function {.justify}

<!-- then i created a function in order to make an api request based on the information/aliases i wanted to search and the dates and sources -->

<!-- explain code .... -->

```{.python code-line-numbers="1-8|9-20|21-25"}
def api_request(search, websites, date):
    """
    search: expression/word (what to look for)
    websites: comma separated websites (where to look for)
    date: list such as [20030101, 20031231] (when to look for)
    -
    returns the response_items from arquivo.pt api
    """
    search = f"q=%22{search.replace(' ', '%20')}%22"
    websites = f"&siteSearch={websites}"
    date = f"&from={date[0]}&to={date[1]}"    
    url = (
        f"https://arquivo.pt/textsearch?{search}{websites}{date}"
        "&fields=linkToArchive,linkToExtractedText,tstamp"
        "&maxItems=500"
        "&dedupValue=25"
        "&dedupField=url"
        "&prettyPrint=false"
        "&type=html"
        )
    json = requests.get(url).json()
    data = json["response_items"]
    if len(data) == 500:
        print(f"You might have lost some data: {search, date}")
    return data
```

## API Request {.justify}

<!-- grupos de 3 para n ter mtas colunas e para todos as colunas terem dados
torna mais facil analisar à mao celulas para encontrar problemas
explicar codigo -->

```{.python code-line-numbers="1-8|9-17|19-35|39-45"} 
def data01(companies):
    """
    Apply the API request for each company. Save the data into a Parquet file.
    
    - companies (dict): A dictionary of companies with their aliases.
    - companies_Ex = {"Galp Energia": ["Galp Energia", "GALP"],
                      "EDP": ["EDP", "Energias de Portugal"]}
    """
    # CREATING DF WITH COMPANIES AND THEIR ALIASES
    companies_data = {"companies": [], "aliases": []}
    for company in companies.keys():
        companies_data["companies"].append(company)
        companies_data["aliases"].append(companies[company])
    df = pd.DataFrame(companies_data).set_index("companies")

    # SITES OF WHERE TO LOOK FOR NEWS
    websites = news()

    # INITIALIZAING API REQUESTS
    # groups of 3 years, from 2000 to 2020
    for cluster in range(2000, 2021, 3):
        api_cluster = [] #reset api_cluster for each cluster (group of 3 year)
        print(f"Processing cluster: {cluster}")
        print("Processing company:", end=" ")
        # iterate over each company
        for company_aliases in df["aliases"]:
            api_company = [] #reset api_company for each company
            print(f"{company_aliases[0]}", end = "; ")
            # iterate over each company's aliases
            for alias in company_aliases:
                # iterate over each cluter's year
                for year in range(cluster, cluster + 3):                        
                    api_aliasS1 = api_request(alias, websites, [int(f"{year}0101"), int(f"{year}0630")])
                    api_aliasS2 = api_request(alias, websites, [int(f"{year}0701"), int(f"{year}1231")])
                    api_company += api_aliasS1 + api_aliasS2
            # save company data
            api_cluster.append(api_company)

        # save cluster into df
        df[f"api.{cluster}"] = api_cluster
        print(f"{cluster} OK.")

    # save all data
    df.to_parquet("data01.parquet")
    print("Finished.")
```


## data01.parquet {.justify}

::: {.center}

```{.python}
pd.read_parquet("data01.parquet")
```


```{=html}
<table style="border: 1px solid #ddd;" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>aliases</th>
<th>api.2000</th>
<th>api.2003</th>
<th>api.2006</th>
<th>api.2009</th>
<th>api.2012</th>
<th>api.2015</th>
<th>api.2018</th>
</tr>
<tr>
<th>companies</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>Banco Comercial Português</th>
<td>[Banco Comercial Português, BCP]</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
</tr>
<tr>
<th>Galp Energia</th>
<td>[Galp Energia, GALP]</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
</tr>
<tr>
<th>EDP</th>
<td>[EDP, Energias de Portu...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
</tr>
<tr>
<th>Sonae</th>
<td>[Sonae, SON]</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
</tr>
<tr>
<th>Mota-Engil</th>
<td>[Mota-Engil, EGL]</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
</tr>
</tbody>
</table>
```

:::


## data01.parquet {.justify}

```{.python}
pd.read_parquet("data01.parquet").map(len)
```

```{=html}
<table class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>aliases</th>
<th>api.2000</th>
<th>api.2003</th>
<th>api.2006</th>
<th>api.2009</th>
<th>api.2012</th>
<th>api.2015</th>
<th>api.2018</th>
</tr>
<tr>
<th>companies</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>Banco Comercial Português</th>
<td style="text-align: right">2</td>
<td style="text-align: right">153</td>
<td style="text-align: right">241</td>
<td style="text-align: right">183</td>
<td style="text-align: right">561</td>
<td style="text-align: right">1074</td>
<td style="text-align: right">1430</td>
<td style="text-align: right">954</td>
</tr>
<tr>
<th>Galp Energia</th>
<td>2</td>
<td>128</td>
<td>389</td>
<td>272</td>
<td>582</td>
<td>1156</td>
<td>1391</td>
<td>968</td>
</tr>
<tr>
<th>EDP</th>
<td>3</td>
<td>133</td>
<td>339</td>
<td>173</td>
<td>653</td>
<td>1232</td>
<td>1970</td>
<td>1096</td>
</tr>
<tr>
<th>Sonae</th>
<td>2</td>
<td>192</td>
<td>435</td>
<td>279</td>
<td>502</td>
<td>1215</td>
<td>1705</td>
<td>1196</td>
</tr>
<tr>
<th>Mota-Engil</th>
<td>2</td>
<td>4</td>
<td>83</td>
<td>60</td>
<td>195</td>
<td>538</td>
<td>828</td>
<td>560</td>
</tr>
</tbody>
</table>
```

$\ $

```{.python}
pd.read_parquet("data01.parquet").iloc[0,1][0]
```
```{python, echo=FALSE}
import json

# Your original dictionary
data = pd.read_parquet("data01.parquet").iloc[0,1][0]

# Convert to JSON string with separators
json_string = json.dumps(data, separators=(',', ':'))

pretty_json_string = json.dumps(data, indent=4)


# Print the JSON string
print(pretty_json_string)
```



# Text Extraction

## Problems Faced {.justify}

- Duplicated texts due to `&dedupValue=25&dedupField=url`

- Extracted text without any aliases

- API usage limit (250 requests/min, error 429)

- API error 404 for certain URLs


## Text Extraction Function {.justify}

<!-- in order to face some of those problems ... -->

```{.python code-line-numbers="1-5|7-10|12-15|17-19|21-24"}
def extracText(linkToExtractedText):
    # infinite loop to handle retries
    while True:
        response = requests.get(linkToExtractedText)
        status_code = response.status_code
        
        # request is successful (200 OK)
        if status_code == 200:
            soup = BeautifulSoup(response.content, "html.parser")
            return soup.get_text()
        
        # too many requests (429 WAIT)
        elif status_code == 429:
            print(" (...)", end = "")
            time.sleep(60)
        
        # no text to be found (404 PASS)
        elif status_code == 404:
            return 0
        
        # something else (???)
        else:
            print(f"Request failed: {status_code}; Link was {linkToExtractedText}")
            break
```

## Applying Text Extraction {.justify}

<!-- because it takes a long to time to extract all the text (around 2h for me) i made a function to apply the extraticon to each column at a time so i could stop any minute or if i got an error dont need to do everything or if i only wanted to process a col to see how it comes out 

select which cols to process and skip the ones already done
-->

```{.python code-line-numbers="1-7|8-10|12-27|29-30|33-36|37-44|46-59"}
def filterColumn(column, aliases):
    """
    extract text for each cell in a column
        - check for duplicated text
        - check for api errors
        - check for aliases in text
    """
    filtered_column = []
    for row in aliases.index:
        print(f"; {row}", end = "")
        
        seen_text = set()
        filtered_cell = []
        for i in column.loc[row]:
            # extract text from linkToExtractedText
            text = extracText(i['linkToExtractedText'])
            
            # skip duplicates or error 404
            if (text in seen_text) or (not text):
                continue
              
            # check for aliases in text
            elif any(alias.lower() in text.lower() for alias in aliases.loc[row]):
                i["ExtractedText"] = text
                i.pop('linkToExtractedText', None)
                filtered_cell.append(i) # save to cell
                seen_text.add(text) # mark as seen

        filtered_column.append(filtered_cell)
    return filtered_column


def processColumns(cols2Process):
    """
    cols2Process = ["api.2000", "api.2003"]
    """
    print(f"Starting: {datetime.now()}")
    try:
        # try to access the started extraction
        df = pd.read_parquet("data02.parquet")
    except:
       # if fails, start a new one
       df = pd.read_parquet("data01.parquet").to_parquet("data02.parquet")
       df = pd.read_parquet("data02.parquet")
       
    for column in cols2Process:
      
        # verify if the column has already been processed
        has_link = "linkToExtractedText" in df.iloc[-1][column][-1]
        has_extracText = "ExtractedText" in df.iloc[-1][column][-1]
        if not has_link and has_extracText:
            print(f"\n{column} already done. Skipping.")
        
        # if not, process the column and save it
        else:
            print(f"\nProcessing {column}", end = ": ")
            df[column] = filterColumn(df[column], df["aliases"])
            df.to_parquet("data02.parquet")
    print(f"\nEnded: {datetime.now()}.")

processColumns(["api.2000", "api.2003", "api.2006", "api.2009", "api.2012", "api.2015", "api.2018"])
```


## data02.parquet {.justify}

<!--the only difference is the lists content (dicitionary) and the qty of news-->

```{.python}
pd.read_parquet("data02.parquet").map(len) - pd.read_parquet("data01.parquet").map(len)
```

```{=html}
<table class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>aliases</th>
<th>api.2000</th>
<th>api.2003</th>
<th>api.2006</th>
<th>api.2009</th>
<th>api.2012</th>
<th>api.2015</th>
<th>api.2018</th>
</tr>
<tr>
<th>companies</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>Banco Comercial Português</th>
<td style="text-align: right">0</td>
<td style="text-align: right">-63</td>
<td style="text-align: right">-50</td>
<td style="text-align: right">-14</td>
<td style="text-align: right">-64</td>
<td style="text-align: right">-91</td>
<td style="text-align: right">-211</td>
<td style="text-align: right">-130</td>
</tr>
<tr>
<th>Galp Energia</th>
<td>0</td>
<td>-62</td>
<td>-156</td>
<td>-91</td>
<td>-113</td>
<td>-192</td>
<td>-287</td>
<td>-156</td>
</tr>
<tr>
<th>EDP</th>
<td>0</td>
<td>-53</td>
<td>-94</td>
<td>-33</td>
<td>-115</td>
<td>-156</td>
<td>-442</td>
<td>-224</td>
</tr>
<tr>
<th>Sonae</th>
<td>0</td>
<td>-62</td>
<td>-117</td>
<td>-40</td>
<td>-43</td>
<td>-106</td>
<td>-305</td>
<td>-170</td>
</tr>
<tr>
<th>Mota-Engil</th>
<td>0</td>
<td>-1</td>
<td>-16</td>
<td>-34</td>
<td>-31</td>
<td>-154</td>
<td>-232</td>
<td>-115</td>
</tr>
</tbody>
</table>
```

$\ $

```{.python}
pd.read_parquet("data02.parquet").iloc[0,1][0]
```

```{python, echo=FALSE}
import json

import unicodedata

def normalize_text(text):
    # Normalize to NFKD and then encode to ASCII ignoring errors
    normalized = unicodedata.normalize('NFKD', text)
    return normalized.encode('ASCII', 'ignore').decode('utf-8')

# Your original dictionary
dict = pd.read_parquet("data02.parquet").iloc[0,1][0]


data = {"tstamp": dict["tstamp"],
            "linkToArchive": dict["linkToArchive"],
            "ExtractedText": normalize_text(dict["ExtractedText"])}

# Convert to JSON string with separators
pretty_json_string = json.dumps(data, indent=4)


# Print the JSON string
print(pretty_json_string)
```


# Data Filtering

## Problems Faced {.justify}


- I've already removed exact duplicate files, but some files remain that are about 90% similar. These should also be removed.

- There are also "news" items that aren't actually relevant to the company: they might be ads, irrelevant financial reports, or other unrelated content. These could be filtered out as well.

Since the first problem is more complex ($O(n^2)$), removing irrelevant news will reduce the dataset and make solving the first problem faster.

$\ $

Note: it would have been ideal to filter these out during text extraction.

## detecting not news

i had to read a bunch of news (610) to create a dataset so i could train a model e and also choose a bunch of feautres, which gave me the following datasetp

```{.python}
pd.read_csv("dtree01.csv").info()
```
```{python, echo=FALSE}
df = pd.read_csv("dtree01.csv", index_col=0)
df["alias_in_url"] = [bool(x) for x in df["alias_in_url"]]
df["news"] = [bool(x) for x in df["news"]]
df.info()
```

## training the decision tree

want to train a decision tree because "do not require linear relationships and can handle non-linear interactions between features. If there are complex, non-linear interactions in your data, a Decision Tree would likely perform better." 

```{.python}
dataset = pd.read_csv("dtree01.csv")
X = dataset[['IstALIAS', 'propAN', 'txtSZ', 'countALI', 'countDTS', 'countHOUR', 'countCAPS']]
y = dataset['news'] 

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=21)


clf = DecisionTreeClassifier(random_state=21, max_depth=5, min_samples_split=15, min_samples_leaf=10)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
```

## Decision Tree

```{python, echo=FALSE}
print("""Accuracy after tuning: 0.9130434782608695
Classification Report: precision    recall  f1-score   support
                    0       0.92      0.97      0.94        68
                    1       0.90      0.75      0.82        24
  
             accuracy                           0.91        92
            macro avg       0.91      0.86      0.88        92
         weighted avg       0.91      0.91      0.91        92""")
```

```{=html}
<figure style="text-align: center;">
    <img style="margin-top: 25px;" src="dtree01.svg" alt="decisiton tree ramos" style="width: 100%;">
    <figcaption style="font-size: 0.45em; margin-top: -22px;">Fig. 1: Decision tree visualization.</figcaption>
</figure>
```

## dtree percentagens distribution

i ran the decision tree in all my data one time to check which would be the distribution, to see how many news i would be left with and found this, so ....

- if newsProb in [.0, .4[ - trash

- if newsProb in [.4, .6] - filter setences: keep only the ones with any alias

- if newsProb in ].6, 1.] - keep everything

```{=html}
<figure style="text-align: center;">
    <img src="dtree01 (percentages).svg" alt="dtree percentagens distribution" style="width: 100%;">
    <figcaption style="font-size: 0.45em; margin-top: -40px; margin-bottom: 25px;">Fig. 2: Percentage distribution of classification 1 in the decision tree model applied to the entire dataset.</figcaption>
</figure>
```

## applying dtree to the dataset

```{.python}
# FEATURES
# added "aliases" to all func in order to run them all without worrying about the inputs
def IstALIAS(text, aliases):
    """where does the first alias appear, title?"""
    indexs = []
    for alias in aliases:
        index = text.lower().find(alias.lower())
        if index != -1:
            indexs.append(index)
    return text[:min(indexs)].count(' ')

def propAN(text, aliases):
    """proportion of alphanumeric chars in the text"""
    alphanumeric_chars = sum(char.isalnum() for char in text)
    proportion = alphanumeric_chars / len(text)
    return proportion

def txtSZ(text, aliases):
    """text size"""
    return len(text)

def countALI(text, aliases):
    """count how many aliases appear in the text"""
    alias_count = {expression: 0 for expression in aliases}
    for alias in aliases:
        # Use re.escape to handle any special characters in the expression
        pattern = re.escape(alias.lower())
        matches = re.findall(alias, text.lower())
        alias_count[alias] = len(matches)
    return sum(alias_count.values())

def countDTS(text, aliases):
    """count how many dates appear in the text"""
    date_pattern = r'\b(\d{1,2}[-/]\d{1,2}[-/]\d{2,4}|\d{4}[-/]\d{1,2}[-/]\d{1,2})\b'
    # 10/11/2024', '10/10/2024', '12-25-1990', '2024-11-05', '01/10/2024'
    dates = re.findall(date_pattern, text)
    date_count = len(dates)
    return date_count

def countHOUR(text, aliases):
    """count how many hours (ex.: hh:mm) appear in the text"""
    time_pattern = r'\b([01]?[0-9]|2[0-3]):[0-5][0-9]\b'   
    occurrences = re.findall(time_pattern, text)
    return len(occurrences)

def countCAPS(text, aliases):
    """count how many WORDS are upper"""
    words = text.split()
    uppercase_word_count = sum(1 for word in words if word.isupper())
    return uppercase_word_count

# 0.4 to 0.6 setences filter
def filter_sentences_by_keywords(text, aliases):
    # Split the text by punctuation and also by multiple spaces or newlines
    sentences = re.split(r'(?<=[.!?]) +|\s{2,}|\n+', text)
    # Filter sentences that contain any of the aliases
    filtered_sentences = [sentence for sentence in sentences if any(keyword.lower() in sentence.lower() for keyword in aliases)]
    # Join the filtered sentences back into a single string
    filtered_text = ' '.join(filtered_sentences)
    return filtered_text

# Load the dtree01 model and set everything up
clf = load('dtree01.joblib')
data = pd.read_parquet("data02.parquet")
features = ['IstALIAS', 'propAN', 'txtSZ', 'countALI', 'countDTS', 'countHOUR', 'countCAPS']

# Applying the model
for row in data.index:
    print(f"\n {row}", end = ": ")
    aliases = data.loc[row, "aliases"]
    for column in data.columns[1:]:
        print(column, end = " | ")
        validation = []
        for req in data.loc[row, column]:
            text = req["ExtractedText"]
            df = {}
            for feature in features:
                df[feature] = [globals()[feature](text, aliases)]
            probability = clf.predict_proba(pd.DataFrame(df))[0, 1]
            if probability < 0.4:
                pass
            elif probability >= 0.4 and probability <= 0.6:
                req["newsProbability"] = round(probability, 3)
                req["ExtractedText"] = filter_sentences_by_keywords(text, aliases)
                validation.append(req)
            elif probability > 0.6:
                req["newsProbability"] = round(probability, 3)
                validation.append(req)
        data.loc[row, column] = validation

# Sava the results
data.to_parquet("data03.parquet")
```

## data03.parquet

the only difference is the lists content (dicitionary) and the qty of news

```{.python}
pd.read_parquet("data03.parquet").map(len) - pd.read_parquet("data02.parquet").map(len)
```

```{=html}
<table class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>aliases</th>
<th>api.2000</th>
<th>api.2003</th>
<th>api.2006</th>
<th>api.2009</th>
<th>api.2012</th>
<th>api.2015</th>
<th>api.2018</th>
</tr>
<tr>
<th>companies</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>Banco Comercial Português</th>
<td style="text-align: right">0</td>
<td style="text-align: right">-77</td>
<td style="text-align: right">-157</td>
<td style="text-align: right">-155</td>
<td style="text-align: right">-350</td>
<td style="text-align: right">-677</td>
<td style="text-align: right">-832</td>
<td style="text-align: right">-441</td>
</tr>
<tr>
<th>Galp Energia</th>
<td>0</td>
<td>-61</td>
<td>-190</td>
<td>-172</td>
<td>-321</td>
<td>-629</td>
<td>-681</td>
<td>-437</td>
</tr>
<tr>
<th>EDP</th>
<td>0</td>
<td>-69</td>
<td>-195</td>
<td>-122</td>
<td>-362</td>
<td>-779</td>
<td>-1064</td>
<td>-432</td>
</tr>
<tr>
<th>Sonae</th>
<td>0</td>
<td>-122</td>
<td>-297</td>
<td>-230</td>
<td>-369</td>
<td>-968</td>
<td>-1071</td>
<td>-725</td>
</tr>
<tr>
<th>Mota-Engil</th>
<td>0</td>
<td>-3</td>
<td>-63</td>
<td>-23</td>
<td>-117</td>
<td>-285</td>
<td>-470</td>
<td>-265</td>
</tr>
</tbody>
</table>
```

```{python}
pd.read_parquet("data03.parquet").iloc[0,1][0]
```

## second problem (90% duplicate)

i need to compare all texs with each other to know which are more than 90% look a like

also i will be merging all years into one col since the data is know less (so i can compare easily the texts of all years)

add news source into the dicitionary

and convert timestamp to only YYYYMM because its the timesatmp of the snapshot and not of the new, and this way i can get a better idea when the new was publish

## merging cols, timestamp and source

```{.python}
df = pd.read_parquet("data03.parquet").map(lambda x: list(x))
df['news'] = df.iloc[:, 1:].sum(axis=1)
df_filtered = df.iloc[:, [0, -1]]
df_filtered
```

```{.python}
def tstampANDsource(lista):
    new_list = []
    noticias = pd.read_csv("noticias.csv")
    for req in lista:
        # news source
        linkToArchive = req["linkToArchive"]
        foundSource = False
        for index, row in noticias.iterrows():
            if row.iloc[0] in linkToArchive:
                req["newsSource"] = row.iloc[1]
                foundSource = True
                break
            else:
                pass
        if not foundSource:
            req["newsSource"] = "unknown"
        # timestamp
        req["tstamp"] = req["tstamp"][:6]
        # SAVE
        new_list.append(req)
    return new_list

df_filtered.loc[:, "news"] = df_filtered["news"].map(lambda x: tstampANDsource(x))
```

## 90% duplicate function

```{.python}
def nearDuplicates(lista, threshold=90):
    total_data = len(lista) # status
    curr_data = 0 # status
    new_list = [lista[0]]
    texts = [lista[0]["ExtractedText"]]
    for req in lista[1:]:
        curr_data += 1 # status
        ExtractedText = req["ExtractedText"]
        similarity = 0
        for txt in texts:
            similarity = max(similarity, fuzz.ratio(txt, ExtractedText))
            if similarity > threshold:
                break
        if similarity <= threshold:
            new_list.append(req)
            texts.append(ExtractedText)
        if random.uniform(0, 1) < 0.01: # status
            print(f"{curr_data} of {total_data}", end = " | ") # status
    print("") # status
    return new_list

df_filtered.loc[:, "news"] = df_filtered["news"].map(lambda x: nearDuplicates(x))
df_filtered.to_parquet("data04.parquet")
```




## data04.parquet

```{=html}
<table style="border: 1px solid #ddd;" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>aliases</th>
<th>news</th>
</tr>
<tr>
<th>companies</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>Banco Comercial Português</th>
<td>[Banco Comercial Português, BCP]</td>
<td>[{'ExtractedText': 'DN &nbsp; 13 de Setembro de 200...</td>
</tr>
<tr>
<th>Galp Energia</th>
<td>[Galp Energia, GALP]</td>
<td>[{'ExtractedText': 'RTP Galp reforça posição n...</td>
</tr>
<tr>
<th>EDP</th>
<td>[EDP, Energias de Portugal, Electricidade de P...</td>
<td>[{'ExtractedText': 'DN-Sinteses Negocios 9 de ...</td>
</tr>
<tr>
<th>Sonae</th>
<td>[Sonae, SON]</td>
<td>[{'ExtractedText': 'DN-Sinteses 5 de Março de ...</td>
</tr>
<tr>
<th>Mota-Engil</th>
<td>[Mota-Engil, EGL]</td>
<td>[{'ExtractedText': 'RTP Lucro da Mota-Engil so...</td>
</tr>
</tbody>
</table>
```

```{python}
pd.read_parquet("data04.parquet").iloc[0,1][0]
```


## data04.parquet {.justify}


```{=html}
<figure style="text-align: center;">
    <img src="lostNews.svg" alt="coemcamos com 100% de news e agora" style="width: 100%;">
    <figcaption style="font-size: 0.45em; margin-top: -25px; margin-bottom: 25px;">Fig. 3: Remaining news after filtering.</figcaption>
</figure>
```

::: {style="text-align: center;"}
Banco Comercial Português $4596 \rightarrow 807$ | Galp Energia $4886 \rightarrow 809$

EDP $5596 \rightarrow 954$ | Sonae $5524 \rightarrow 520$ | Mota-Engil $2268 \rightarrow 296$
:::


# conclusion

## ...

...

## libraries

```{.python}
import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime
import re
import random
import numpy as np
# dtree related
import os
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
from joblib import dump, load
# 90% duplicated
from fuzzywuzzy import fuzz
```


## disclaimer

ofc i could have all this datasets and filtering in one go and by only using one parquet file, but now that i have the strucutre i can easily implement a way so that any given company and aliases returns me the cleaned and filtered news

all the source code can be found in github