---
title: "titulo"
subtitle: "subtitulo?"
author: 
  - Hugo Veríssimo 124348
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: ua.png
    css: mystyle.css
    theme: serif
    transition: slide
echo: true
---

```{r setup, include = FALSE}
# packages
library(dplyr)
library(knitr)
library(xtable)
library(reticulate)
```

```{python, include=FALSE}
import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime
```



# Context and Objective

```{=html}
<style>
.dataframe {
  display: block;
  max-width: 100%;
  max-height: 75%; /* vertical scrolling */
  overflow-x: auto;
  overflow-y: auto;
  font-family: "SFMono-Regular", Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
  /*border: 1px solid #ddd; apply only to big dfs */
}

table {
  max-width: 100%;
  border-collapse: collapse;
}

th, td { /* th is about header*/
  padding: 8px 16px;
  border: 1px solid #ddd; /* Border between cells */
  text-align: left;
  vertical-align: middle;
  font-size: 16px;
}

thead th {
  background-color: rgba(128, 128, 128, 0.3);
  font-weight: bold;
}

tbody td:first-child {
  background-color: rgba(128, 128, 128, 0.3);
  font-weight: bold;
}
</style>
```

## . {.justify}

- intro qq

- o meu objetivo

- trabalhos parecidos

**Objective:** Based on the names and aliases of various companies, find multiple mentions of these companies in news articles and attempt to:

1Create a graph of words/people/topics/associated companies (and try to determine if the term/person is good/bad for the company)

  Try to analyze the source of the relationship (e.g., Público: 3 mentions, TVI: 10 mentions, ...)

2Explore the relationship between news (positive/negative) and stock prices.

3...

## .

Trabalho tem de ter 3 partes:

1. project structure + data acquisition

2. exploratory data analysis and visualization

3. results & discussion

Fonte de Dados: arquivo.pt (https://github.com/arquivo/pwa-technologies/wiki/Arquivo.pt-API)

Ideias semelhantes:

- https://github.com/politiquices

- https://github.com/msramalho/desarquivo

## libraries

```{.python}
import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime
import re
import random
import numpy as np
# dtree related
import os
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
from joblib import dump, load
# 90% duplicated
from fuzzywuzzy import fuzz
```


# Data Extraction from arquivo.pt

## companies {.justify}

first i had to select which companies i would be analyzing and set their aliases (names that they are known for) from psi20 selected 5 more or less randomly

```{python}
companies = {"Banco Comercial Português": ["Banco Comercial Português", "BCP"],
             "Galp Energia": ["Galp Energia", "GALP"],
             "EDP": ["EDP", "Energias de Portugal", "Electricidade de Portugal"],
             "Sonae": ["Sonae", "SON"],
             "Mota-Engil": ["Mota-Engil", "EGL"]}
```




## News Sources Selection {.justify}    

```{.python}
pd.read_csv('noticias.csv')
```

```{python, echo=FALSE}
pd.read_csv('noticias.csv', header=None)
```

```{python}
def news(csvFile = 'noticias.csv'):
    """
    grab the news websites from a csv file
    """
    links = pd.read_csv(csvFile, header=None).iloc[:,0]
    return ",".join(links)
```

## API Request Function

```{.python code-line-numbers="1-8|9-20|21-25"}
def api_request(search, websites, date):
    """
    search: expression/word (what to look for)
    websites: comma separated websites (where to look for)
    date: list such as [20030101, 20031231] (when to look for)
    -
    returns the responde_items from arquivo.pt api
    """
    search = f"q=%22{search.replace(' ', '%20')}%22"
    websites = f"&siteSearch={websites}"
    date = f"&from={date[0]}&to={date[1]}"    
    url = (
        f"https://arquivo.pt/textsearch?{search}{websites}{date}"
        "&fields=linkToArchive,linkToExtractedText,tstamp"
        "&maxItems=500"
        "&dedupValue=25"
        "&dedupField=url"
        "&prettyPrint=false"
        "&type=html"
        )
    json = requests.get(url).json()
    data = json["response_items"]
    if len(data) == 500:
        print(f"You might have lost some data: {search, date}")
    return data
```

## API Request

grupos de 3 para n ter mtas colunas e para todos as colunas terem dados

torna mais facil analisar à mao celulas para encontrar problemas

```{.python code-line-numbers="1-8|9-20|21-40"} 
# POR FAZER
def datav1(companies):
    """
    Select companies for extraction and save the data into a Parquet file.
    
    - companies (dict): A dictionary of companies with their aliases.
    - companies_Ex = {"Galp Energia": ["Galp Energia", "GALP"],
                      "EDP": ["EDP", "Energias de Portugal"]}
    """
    # CREATING DF WITH COMPANIES AND THEIR ALIASES
    companies_data = {"companies": [], "aliases": []}
    for company in companies.keys():
        companies_data["companies"].append(company)
        companies_data["aliases"].append(companies[company])
    df = pd.DataFrame(companies_data).set_index("companies")

    # SITES OF WHERE TO LOOK FOR NEWS
    websites = news()

    # INITIALIZAING API REQUESTS
    # groups of 3 years, from 2000 to 2020
    for cluster in range(2000, 2021, 3):
        api_cluster = [] #reset api_cluster for each cluster (group of 3 year)
        print(f"Processing cluster: {cluster}")
        print("Processing company:", end=" ")
        # iterate over each company
        for company_aliases in df["aliases"]:
            api_company = [] #reset api_company for each company
            print(f"{company_aliases[0]}", end = "; ")
            # iterate over each company's aliases
            for alias in company_aliases:
                # iterate over each cluter's year
                for year in range(cluster, cluster + 3):                        
                    api_aliasS1 = api_request(alias, websites, [int(f"{year}0101"), int(f"{year}0630")])
                    api_aliasS2 = api_request(alias, websites, [int(f"{year}0701"), int(f"{year}1231")])
                    api_company += api_aliasS1 + api_aliasS2
            # save company data
            api_cluster.append(api_company)

        # save cluster (group of 3 years) data
        df[f"api.{cluster}"] = api_cluster
        print(f"{cluster} OK.")

    # save all data
    df.to_parquet("data01.parquet")
    print("Finished.")
```


## data01.parquet

```{=html}
<table style="border: 1px solid #ddd;" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>aliases</th>
<th>api.2000</th>
<th>api.2003</th>
<th>api.2006</th>
<th>api.2009</th>
<th>api.2012</th>
<th>api.2015</th>
<th>api.2018</th>
</tr>
<tr>
<th>companies</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>Banco Comercial Português</th>
<td>[Banco Comercial Português, BCP]</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
</tr>
<tr>
<th>Galp Energia</th>
<td>[Galp Energia, GALP]</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
</tr>
<tr>
<th>EDP</th>
<td>[EDP, Energias de Portu...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
</tr>
<tr>
<th>Sonae</th>
<td>[Sonae, SON]</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
</tr>
<tr>
<th>Mota-Engil</th>
<td>[Mota-Engil, EGL]</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
<td>[{'linkToArchive': 'https://arquivo.pt/wayback...</td>
</tr>
</tbody>
</table>
```

## a

```{.python}
pd.read_parquet("data01.parquet").map(len)
```

```{=html}
<table class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>aliases</th>
<th>api.2000</th>
<th>api.2003</th>
<th>api.2006</th>
<th>api.2009</th>
<th>api.2012</th>
<th>api.2015</th>
<th>api.2018</th>
</tr>
<tr>
<th>companies</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>Banco Comercial Português</th>
<td style="text-align: right">2</td>
<td style="text-align: right">153</td>
<td style="text-align: right">241</td>
<td style="text-align: right">183</td>
<td style="text-align: right">561</td>
<td style="text-align: right">1074</td>
<td style="text-align: right">1430</td>
<td style="text-align: right">954</td>
</tr>
<tr>
<th>Galp Energia</th>
<td>2</td>
<td>128</td>
<td>389</td>
<td>272</td>
<td>582</td>
<td>1156</td>
<td>1391</td>
<td>968</td>
</tr>
<tr>
<th>EDP</th>
<td>3</td>
<td>133</td>
<td>339</td>
<td>173</td>
<td>653</td>
<td>1232</td>
<td>1970</td>
<td>1096</td>
</tr>
<tr>
<th>Sonae</th>
<td>2</td>
<td>192</td>
<td>435</td>
<td>279</td>
<td>502</td>
<td>1215</td>
<td>1705</td>
<td>1196</td>
</tr>
<tr>
<th>Mota-Engil</th>
<td>2</td>
<td>4</td>
<td>83</td>
<td>60</td>
<td>195</td>
<td>538</td>
<td>828</td>
<td>560</td>
</tr>
</tbody>
</table>
```



```{python}
#pd.read_parquet("data01.parquet")["api.2000"].loc["Mota-Engil"]
pd.read_parquet("data01.parquet").iloc[0,1][0]
```



# text extratcion


## problems

por ter usado `&dedupValue=25&dedupField=url` e diferentes aliases, há informação repetida

**filtrar repetidos e textos que não mencionem nenhum alias**

problemas ultrapassados:

- API has the following usage limits (250req/min, error 429): `time.sleep(60)`

- API error 404 for some urls: return 0 (False) and skip it

- extrair o texto demora muito: filtrar e salvar coluna a coluna

nota: podia ter feito já online o processamento do texto, mas não queria estar dependente do wifi


## extract text function

```{python}
def extracText(linkToExtractedText):
    # Infinite loop to handle retry logic in case of 429 Too Many Requests
    while True:
        response = requests.get(linkToExtractedText)
        status_code = response.status_code
        
        if status_code == 200:
            # If the request is successful (200 OK), return the extracted text
            soup = BeautifulSoup(response.content, "html.parser")
            return soup.get_text()
        elif status_code == 429:
            # Handle 429 Too Many Requests by reading the Retry-After header
            print(" (...)", end = "")
            time.sleep(60)  # Pause execution for the retry period
        elif status_code == 404:
            return 0
        else:
            # For any other status codes (e.g., 500, ...), print the status and break the loop
            print(f"Request failed: {status_code}; Link was {linkToExtractedText}")
            break
```

## applying it

because it takes a long to time to extract all the text (around 2h for me) i made a function to apply the extraticon to each column at a time so i could stop any minute or if i got an error dont need to do everything or if i only wanted to process a col to see how it comes out

```{python}
# Function to process each column
def filterColumn(column, aliases):
    """aliases in text, repeated text and extract text"""
    filtered_column = []

    for row in aliases.index:
        filtered_cell = []
        seen_text = set()
        print(f"; {row}", end = "")
        for i in column.loc[row]:
            
            # Extract text from 'linkToExtractedText'
            text = extracText(i['linkToExtractedText'])
            
            # Skip if the text has already been processed
            if text in seen_text:
                continue

            elif not text: #ERROR 404
                continue
            
            # Check if any alias is found in the text
            elif any(alias.lower() in text.lower() for alias in aliases.loc[row]):
                i["ExtractedText"] = text  # Add extracted text to the record
                
                # Remove unwanted fields
                i.pop('linkToExtractedText', None)
                
                # Append the processed record
                filtered_cell.append(i)
                
                # Mark this text as processed
                seen_text.add(text)

        filtered_column.append(filtered_cell)
                
    return filtered_column
```

## applying it pt2

select which cols to process and skip the ones already done

```{python}
def processColumns(col_to_proc):
    print(f"Starting: {datetime.now()}")
    try:
        # continuar df criada
        df = pd.read_parquet("data02.parquet")
    except:
       # criar df para trabalhar
       df = pd.read_parquet("data01.parquet").to_parquet("data02.parquet")
       df = pd.read_parquet("data02.parquet")
    for column in col_to_proc:
        has_link = "linkToExtractedText" in df.iloc[-1][column][-1]
        has_extracText = "ExtractedText" in df.iloc[-1][column][-1]
        if not has_link and has_extracText:
            print(f"\n{column} already done. Skipping.")
        else:
            print(f"\nProcessing {column}", end = ": ")
            df[column] = filterColumn(df[column], df["aliases"])
            df.to_parquet("data02.parquet")
    print(f"\nEnded: {datetime.now()}.")

#processColumns(["api.2000", "api.2003", "api.2006", "api.2009", "api.2012", "api.2015", "api.2018"])
```



## data02.parquet

the only difference is the lists content (dicitionary) and the qty of news

```{.python}
pd.read_parquet("data02.parquet").map(lambda x: len(x)) - pd.read_parquet("data01.parquet").map(lambda x: len(x))
```

```{=html}
<table class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>aliases</th>
<th>api.2000</th>
<th>api.2003</th>
<th>api.2006</th>
<th>api.2009</th>
<th>api.2012</th>
<th>api.2015</th>
<th>api.2018</th>
</tr>
<tr>
<th>companies</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>Banco Comercial Português</th>
<td style="text-align: right">0</td>
<td style="text-align: right">-63</td>
<td style="text-align: right">-50</td>
<td style="text-align: right">-14</td>
<td style="text-align: right">-64</td>
<td style="text-align: right">-91</td>
<td style="text-align: right">-211</td>
<td style="text-align: right">-130</td>
</tr>
<tr>
<th>Galp Energia</th>
<td>0</td>
<td>-62</td>
<td>-156</td>
<td>-91</td>
<td>-113</td>
<td>-192</td>
<td>-287</td>
<td>-156</td>
</tr>
<tr>
<th>EDP</th>
<td>0</td>
<td>-53</td>
<td>-94</td>
<td>-33</td>
<td>-115</td>
<td>-156</td>
<td>-442</td>
<td>-224</td>
</tr>
<tr>
<th>Sonae</th>
<td>0</td>
<td>-62</td>
<td>-117</td>
<td>-40</td>
<td>-43</td>
<td>-106</td>
<td>-305</td>
<td>-170</td>
</tr>
<tr>
<th>Mota-Engil</th>
<td>0</td>
<td>-1</td>
<td>-16</td>
<td>-34</td>
<td>-31</td>
<td>-154</td>
<td>-232</td>
<td>-115</td>
</tr>
</tbody>
</table>
```

```{.python}
pd.read_parquet("data02.parquet").iloc[0,1][0]
```

```{python, echo=TRUE}
dict = pd.read_parquet("data02.parquet").iloc[0,1][0]
new_dict = {"tstamp": dict["tstamp"],
            "linkToArchive": dict["linkToArchive"],
            "ExtractedText": dict["ExtractedText"]}
            
new_dict # create function to read dictionarys
```


# data filtering

## problems

- i already removes duplicates files but there are some that are like 90% ducplicated, i should remove them

- there are "news" that are actualy ads or not related to the company, like it just appears there for another reason, like to say its stock went down just like a bunch of psi20 companies

## first problem (problem 2 actualy)

i will solve this one first because it will allow me to compare less text with each other when trying to remove 90% neaer duplicates, so its about complexety

for that i m training want to train a decision tree because "do not require linear relationships and can handle non-linear interactions between features. If there are complex, non-linear interactions in your data, a Decision Tree would likely perform better."

i had to read a bunch of news (610) to create a dataset so i could train the decision tree and also choose a bunch of feautres, which gave me the following datasetp

```{.python}
pd.read_csv("dtree01.csv").info()
```
```{python, echo=FALSE}
df = pd.read_csv("dtree01.csv", index_col=0)
df["alias_in_url"] = [bool(x) for x in df["alias_in_url"]]
df["news"] = [bool(x) for x in df["news"]]
df.info()
```

## training the decision tree

```{.python}
dataset = pd.read_csv("dtree01.csv")
X = dataset[['IstALIAS', 'propAN', 'txtSZ', 'countALI', 'countDTS', 'countHOUR', 'countCAPS']]
y = dataset['news'] 

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=21)


clf = DecisionTreeClassifier(random_state=21, max_depth=5, min_samples_split=15, min_samples_leaf=10)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
```

## decision tree

```{.python}
print(f"Accuracy after tuning: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report: {classification_report(y_test, y_pred)}")
```
```{python, echo=FALSE}
print("""Accuracy after tuning: 0.9130434782608695
Classification Report: precision    recall  f1-score   support
                    0       0.92      0.97      0.94        68
                    1       0.90      0.75      0.82        24
  
             accuracy                           0.91        92
            macro avg       0.91      0.86      0.88        92
         weighted avg       0.91      0.91      0.91        92""")
```

```{=html}
<div style="text-align: center;">
    <img src="dtree01.svg" alt="Description of image" style="width: 100%;">
</div>
```

## dtree percentagens distribution

i ran the decision tree in all my data one time to check which would be the distribution, to see how many news i would be left with and found this, so ....

- if newsProb in [.0, .4[ - trash

- if newsProb in [.4, .6] - filter setences: keep only the ones with any alias

- if newsProb in ].6, 1.] - keep everything

```{=html}
<div style="text-align: center;">
    <img src="dtree01 (percentages).svg" alt="Description of image" style="width: 100%;">
</div>
```

## applying dtree to the dataset

```{.python}
# FEATURES
# added "aliases" to all func in order to run them all without worrying about the inputs
def IstALIAS(text, aliases):
    """where does the first alias appear, title?"""
    indexs = []
    for alias in aliases:
        index = text.lower().find(alias.lower())
        if index != -1:
            indexs.append(index)
    try:
        a = text[:min(indexs)].count(' ')
    except:
        a = 10000000000000000000
    return a

def propAN(text, aliases):
    """proportion of alphanumeric chars in the text"""
    alphanumeric_chars = sum(char.isalnum() for char in text)
    proportion = alphanumeric_chars / len(text)
    return proportion

def txtSZ(text, aliases):
    """text size"""
    return len(text)

def countALI(text, aliases):
    """count how many aliases appear in the text"""
    alias_count = {expression: 0 for expression in aliases}
    for alias in aliases:
        # Use re.escape to handle any special characters in the expression
        pattern = re.escape(alias.lower())
        matches = re.findall(alias, text.lower())
        alias_count[alias] = len(matches)
    return sum(alias_count.values())

def countDTS(text, aliases):
    """count how many dates appear in the text"""
    date_pattern = r'\b(\d{1,2}[-/]\d{1,2}[-/]\d{2,4}|\d{4}[-/]\d{1,2}[-/]\d{1,2})\b'
    # 10/11/2024', '10/10/2024', '12-25-1990', '2024-11-05', '01/10/2024'
    dates = re.findall(date_pattern, text)
    date_count = len(dates)
    return date_count

def countHOUR(text, aliases):
    """count how many hours (ex.: hh:mm) appear in the text"""
    time_pattern = r'\b([01]?[0-9]|2[0-3]):[0-5][0-9]\b'   
    occurrences = re.findall(time_pattern, text)
    return len(occurrences)

def countCAPS(text, aliases):
    """count how many WORDS are upper"""
    words = text.split()
    uppercase_word_count = sum(1 for word in words if word.isupper())
    return uppercase_word_count

# 0.4 to 0.6 setences filter
def filter_sentences_by_keywords(text, aliases):
    # Split the text by punctuation and also by multiple spaces or newlines
    sentences = re.split(r'(?<=[.!?]) +|\s{2,}|\n+', text)
    # Filter sentences that contain any of the aliases
    filtered_sentences = [sentence for sentence in sentences if any(keyword.lower() in sentence.lower() for keyword in aliases)]
    # Join the filtered sentences back into a single string
    filtered_text = ' '.join(filtered_sentences)
    return filtered_text

# Load the dtree01 model and set everything up
clf = load('dtree01.joblib')
data = pd.read_parquet("data02.parquet")
features = ['IstALIAS', 'propAN', 'txtSZ', 'countALI', 'countDTS', 'countHOUR', 'countCAPS']

# Applying the model
for row in data.index:
    print(f"\n {row}", end = ": ")
    aliases = data.loc[row, "aliases"]
    for column in data.columns[1:]:
        print(column, end = " | ")
        validation = []
        for req in data.loc[row, column]:
            text = req["ExtractedText"]
            df = {}
            for feature in features:
                df[feature] = [globals()[feature](text, aliases)]
            #prediction = clf.predict(pd.DataFrame(df)) # binario
            probability = clf.predict_proba(pd.DataFrame(df))[0, 1]
            if probability < 0.4:
                pass
            elif probability >= 0.4 and probability <= 0.6:
                req["newsProbability"] = round(probability, 3)
                req["ExtractedText"] = filter_sentences_by_keywords(text, aliases)
                validation.append(req)
            elif probability > 0.6:
                req["newsProbability"] = round(probability, 3)
                validation.append(req)
        data.loc[row, column] = validation

# Sava the results
data.to_parquet("data03.parquet")
```

## data03.parquet

the only difference is the lists content (dicitionary) and the qty of news

```{.python}
pd.read_parquet("data03.parquet").map(lambda x: len(x)) - pd.read_parquet("data02.parquet").map(lambda x: len(x))
```

```{=html}
<table class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>aliases</th>
<th>api.2000</th>
<th>api.2003</th>
<th>api.2006</th>
<th>api.2009</th>
<th>api.2012</th>
<th>api.2015</th>
<th>api.2018</th>
</tr>
<tr>
<th>companies</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>Banco Comercial Português</th>
<td style="text-align: right">0</td>
<td style="text-align: right">-77</td>
<td style="text-align: right">-157</td>
<td style="text-align: right">-155</td>
<td style="text-align: right">-350</td>
<td style="text-align: right">-677</td>
<td style="text-align: right">-832</td>
<td style="text-align: right">-441</td>
</tr>
<tr>
<th>Galp Energia</th>
<td>0</td>
<td>-61</td>
<td>-190</td>
<td>-172</td>
<td>-321</td>
<td>-629</td>
<td>-681</td>
<td>-437</td>
</tr>
<tr>
<th>EDP</th>
<td>0</td>
<td>-69</td>
<td>-195</td>
<td>-122</td>
<td>-362</td>
<td>-779</td>
<td>-1064</td>
<td>-432</td>
</tr>
<tr>
<th>Sonae</th>
<td>0</td>
<td>-122</td>
<td>-297</td>
<td>-230</td>
<td>-369</td>
<td>-968</td>
<td>-1071</td>
<td>-725</td>
</tr>
<tr>
<th>Mota-Engil</th>
<td>0</td>
<td>-3</td>
<td>-63</td>
<td>-23</td>
<td>-117</td>
<td>-285</td>
<td>-470</td>
<td>-265</td>
</tr>
</tbody>
</table>
```

```{python}
pd.read_parquet("data03.parquet").iloc[0,1][0]
```

## second problem (90% duplicate)

i need to compare all texs with each other to know which are more than 90% look a like

also i will be merging all years into one col since the data is know less (so i can compare easily the texts of all years)

add news source into the dicitionary

and convert timestamp to only YYYYMM because its the timesatmp of the snapshot and not of the new, and this way i can get a better idea when the new was publish

## merging cols, timestamp and source

```{.python}
df = pd.read_parquet("data03.parquet").map(lambda x: list(x))
df['news'] = df.iloc[:, 1:].sum(axis=1)
df_filtered = df.iloc[:, [0, -1]]
df_filtered
```

```{.python}
def tstampANDsource(lista):
    new_list = []
    noticias = pd.read_csv("noticias.csv")
    for req in lista:
        # news source
        linkToArchive = req["linkToArchive"]
        foundSource = False
        for index, row in noticias.iterrows():
            if row.iloc[0] in linkToArchive:
                req["newsSource"] = row.iloc[1]
                foundSource = True
                break
            else:
                pass
        if not foundSource:
            req["newsSource"] = "unknown"
        # timestamp
        req["tstamp"] = req["tstamp"][:6]
        # SAVE
        new_list.append(req)
    return new_list

df_filtered.loc[:, "news"] = df_filtered["news"].map(lambda x: tstampANDsource(x))
```

## 90% duplicate function

```{.python}
def nearDuplicates(lista, threshold=90):
    total_data = len(lista) # status
    curr_data = 0 # status
    new_list = [lista[0]]
    texts = [lista[0]["ExtractedText"]]
    for req in lista[1:]:
        curr_data += 1 # status
        ExtractedText = req["ExtractedText"]
        similarity = 0
        for txt in texts:
            similarity = max(similarity, fuzz.ratio(txt, ExtractedText))
            if similarity > threshold:
                break
        if similarity <= threshold:
            new_list.append(req)
            texts.append(ExtractedText)
        if random.uniform(0, 1) < 0.01: # status
            print(f"{curr_data} of {total_data}", end = " | ") # status
    print("") # status
    return new_list

df_filtered.loc[:, "news"] = df_filtered["news"].map(lambda x: nearDuplicates(x))
df_filtered.to_parquet("data04.parquet")
```




## data04.parquet

```{=html}
<table style="border: 1px solid #ddd;" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>aliases</th>
<th>news</th>
</tr>
<tr>
<th>companies</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>Banco Comercial Português</th>
<td>[Banco Comercial Português, BCP]</td>
<td>[{'ExtractedText': 'DN &nbsp; 13 de Setembro de 200...</td>
</tr>
<tr>
<th>Galp Energia</th>
<td>[Galp Energia, GALP]</td>
<td>[{'ExtractedText': 'RTP Galp reforça posição n...</td>
</tr>
<tr>
<th>EDP</th>
<td>[EDP, Energias de Portugal, Electricidade de P...</td>
<td>[{'ExtractedText': 'DN-Sinteses Negocios 9 de ...</td>
</tr>
<tr>
<th>Sonae</th>
<td>[Sonae, SON]</td>
<td>[{'ExtractedText': 'DN-Sinteses 5 de Março de ...</td>
</tr>
<tr>
<th>Mota-Engil</th>
<td>[Mota-Engil, EGL]</td>
<td>[{'ExtractedText': 'RTP Lucro da Mota-Engil so...</td>
</tr>
</tbody>
</table>
```

```{python}
pd.read_parquet("data04.parquet").iloc[0,1][0]
```


## data04.parquet

compare len + graph

# conclusion

## how many data did i clean

## disclaimer

ofc i could have all this datasets and filtering in one go and by only using one parquet file, but now that i have the strucutre i can easily implement a way so that any given company and aliases returns me the cleaned and filtered news

all the source code can be found in github