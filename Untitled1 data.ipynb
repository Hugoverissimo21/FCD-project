{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**todo**\n",
    "\n",
    "- ~~analise sentimento~~\n",
    "\n",
    "- ~~ner + conceitos~~\n",
    "\n",
    "- ~~tentar newsVSprice de novo, agora com menos noticias~~\n",
    "\n",
    "- ~~qty de noticias por newsSource~~\n",
    "\n",
    "- ~~word cloud com cores de cada empresa~~\n",
    "\n",
    "- ~~adicionei palavras ao set das \"noticias\"... dps tnh q correr isto de novo.........~~\n",
    "\n",
    "- grafo\n",
    "\n",
    "- corr de keywords e stock price (timeseries)\n",
    "\n",
    "- diagrama de ven, pq so temos 3 newsSources para ver q palavras algumas dizem e outras não, ou duas em comum, ...\n",
    "\n",
    "    -   e um venn para cada empresa e ver q palavras cada um tem e em comum e isso - se calhar mais vale o grafo aqui\n",
    "\n",
    "- newsVSprices com sentimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER aos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "df = pd.read_parquet(\"data04.parquet\")\n",
    "\n",
    "# Load the Portuguese model\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "\n",
    "def NER_KEYWORDS(dict):\n",
    "    text = dict[\"ExtractedText\"]\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tempos = {\"janeiro\", \"fevereiro\", \"março\", \"abril\", \"maio\", \"junho\",\n",
    "              \"julho\", \"agosto\", \"setembro\", \"outubro\", \"novembro\", \"dezembro\",\n",
    "              \"segunda-feira\", \"terça-feira\", \"quarta-feira\", \"quinta-feira\",\n",
    "              \"sexta-feira\", \"sábado\", \"domingo\", \"fim-de-semana\"}\n",
    "    lixo = {\"REFER\", \"Pedro Cunha Especial Saramago\", \"Angra do Heroísmo Aveiro Beja\",\n",
    "            \"Braga Bragança Castelo Branco Coimbra\"}\n",
    "    noticias = {\"comentários\", \"notícias\", \"artigo\", \"jornal\", \"assine\", \"facebook\", \"assinaturas\",\n",
    "                \"online\", \"twitter\", \"instagram\", \"linkedin\", \"whatsapp\", \"telegram\", \"youtube\",\n",
    "                \"início\", \"partilhar\", \"contactos\", \"newsletter\", \"enviar\", \"termos\", \"privacidade\",\n",
    "                \"iniciar\", \"inicie\", \"sessão\", \"subscrever\", \"subscreva\", \"subscrever\", \"tv\", \"rádio\",\n",
    "                \"conteúdos\", \"cookies\", \"opinião\", \"assinantes\", \"publicação\", \"edição\", \"edições\",\n",
    "                \"informações\", \"patrocínio\", \"jornalismo\", \"entrevista\", \"lusa\", \"antena\",\n",
    "                \"partilhe\", \"expresso\", \"público\", \"observador\", \"sábado\", \"paypal\", \"limites\",\n",
    "                \"login\", \"vídeo\", \"entre\", \"programação\", \"newsletters\", \"informação\", \"site\",\n",
    "                \"partilhas\", \"notícia\", \"clique\", \"comentário\", \"conteúdo\", \"email\", \"password\",\n",
    "                \"jn\", \"cm\", \"dn\", \"rtp\", \"sic\", \"tvi\", \"limites\", \"índice\", \"imprimir\",\n",
    "                \"perguntas frequentes\", \"utilizador\", \"sair\", \"reportagem\", \"ongoing\",\n",
    "                \"localidade\", \"cinecartaz\", \"meteo\", \"perfil\", \"siga-nos\", \"dados pessoais\",\n",
    "                \" alterar\", \"desporto \", \"pesquisar \"}\n",
    "\n",
    "    \n",
    "    # Extract named entities and filter keywords\n",
    "    named_entities = [\n",
    "        (ent.text, ent.label_) for ent in doc.ents\n",
    "        if ent.label_ in {\"PER\", \"ORG\", \"LOC\", \"MISC\"}\n",
    "        and ent.text.lower() not in tempos\n",
    "        and ent.text not in lixo\n",
    "        and all(w not in ent.text.lower() for w in noticias)\n",
    "        and all(char.isalnum() or char in {\" \", \"-\"} for char in ent.text)\n",
    "        and len(ent.text) <= 50\n",
    "    ]\n",
    "    palavras_significativas = [\n",
    "        token.text for token in doc \n",
    "        if token.pos_ in {\"NOUN\"} # {\"NOUN\", \"VERB\", \"PROPN\"} \n",
    "        and not token.is_stop\n",
    "        and (token.text.islower() or token.text.isupper())\n",
    "        and token.text.lower() not in tempos \n",
    "        and token.text not in lixo \n",
    "        and token.text.lower() not in noticias\n",
    "        and token.text.isalnum()\n",
    "        and len(token.text) > 3\n",
    "    ]\n",
    "\n",
    "    organized_entities = {}\n",
    "    for entity, category in named_entities:\n",
    "        if category not in organized_entities:\n",
    "            organized_entities[category] = [entity]\n",
    "        else:\n",
    "            organized_entities[category].append(entity)\n",
    " \n",
    "    dict[\"newsNER\"] = organized_entities\n",
    "    dict[\"newsNER\"][\"WORD\"] =  palavras_significativas\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analise de sentimentos\n",
    "\n",
    "- pipeline nunca dava, muito complicado\n",
    "\n",
    "- estes dois so dao em ingles e um tem mt em conta os factos, tp terrorimos etc, outro tem os sentimentos de qm fala, ent mais vale fazer uma média"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "def split_text(text, chunk_size=2500):\n",
    "        return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)][:5]\n",
    "\n",
    "def sentimentos(dict):\n",
    "    texto_pt = split_text(dict[\"ExtractedText\"])\n",
    "\n",
    "    en_text = []\n",
    "    for chunk in texto_pt:\n",
    "        traducao = GoogleTranslator(source='pt', target='en').translate(chunk)\n",
    "        en_text.append(traducao)\n",
    "    texto_en = \"\".join([text for text in en_text if text is not None])\n",
    "\n",
    "    # Initialize VADER and TextBlob\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    blob = TextBlob(texto_en)\n",
    "\n",
    "    # Get VADER and TextBlob sentiment scores\n",
    "    vader_scores = analyzer.polarity_scores(texto_en)\n",
    "    textblob_sentiment = blob.sentiment\n",
    "\n",
    "    sentiment = vader_scores[\"compound\"] * 0.6 + textblob_sentiment.polarity * 0.4\n",
    "\n",
    "    dict[\"newsSentiment\"] = sentiment\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# processar coluna das noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def ProcCol(list):\n",
    "    i = 0\n",
    "    for i in range(len(list)):\n",
    "        list[i] = NER_KEYWORDS(list[i])\n",
    "        list[i] = sentimentos(list[i])\n",
    "        if random.random() < 0.02:\n",
    "            print(list[i][\"ExtractedText\"])\n",
    "            print(i, i / len(list))\n",
    "        i += 1\n",
    "    return list\n",
    "\n",
    "try:\n",
    "    df = pd.read_parquet(\"data05.parquet\")\n",
    "except:\n",
    "    df = pd.read_parquet(\"data04.parquet\")\n",
    "\n",
    "df[\"processed_news\"] = None\n",
    "for index, row in df.iterrows():\n",
    "    if df.at[index, \"processed_news\"] == None:\n",
    "        try:\n",
    "            df.at[index, \"processed_news\"] = ProcCol(row[\"news\"])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {index}: {e}\")\n",
    "            df.at[index, \"processed_news\"] = None\n",
    "            break\n",
    "\n",
    "    df.to_parquet(\"data05.parquet\")\n",
    "\n",
    "if df[\"processed_news\"].notna().all():\n",
    "    df[\"news\"] = df[\"processed_news\"]\n",
    "    df.drop(columns=[\"processed_news\"], inplace=True)\n",
    "    df.to_parquet(\"data05.parquet\")\n",
    "    print(\"Done!\")\n",
    "\n",
    "else:\n",
    "    print(\"Not all rows were processed successfully; 'news' column was not updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# criar coluna keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_weight(probNews, type):\n",
    "    weight = 0\n",
    "    # noticia so com frases q têm aliases\n",
    "    if probNews <= 0.6:\n",
    "        weight += 2\n",
    "    elif probNews < 0.9:\n",
    "        weight -= 0.15\n",
    "    if \"PER\" in type:\n",
    "        weight += 1.3\n",
    "    if \"ORG\" in type:\n",
    "        weight += 1.1\n",
    "    if \"LOC\" in type:\n",
    "        weight += 1.4\n",
    "    if \"WORD\" in type:\n",
    "        weight += 0.85\n",
    "    return weight\n",
    "\n",
    "def sentiment_weight(probNews):\n",
    "    if probNews <= 0.6:\n",
    "        return 1.1\n",
    "    elif probNews == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0.95\n",
    "\n",
    "\n",
    "def keywords(lista):\n",
    "    ner = {}\n",
    "    for news in lista:\n",
    "        keywords = news[\"newsNER\"]\n",
    "        sentiment = news[\"newsSentiment\"]\n",
    "        for type, words in keywords.items():\n",
    "            for word in words:\n",
    "                # normalizar word\n",
    "                if word.lower() in ner:\n",
    "                    word = word.lower()\n",
    "                elif word.title() in ner:\n",
    "                    word = word.title()\n",
    "                elif word.upper() in ner:\n",
    "                    word = word.upper()\n",
    "\n",
    "                # adicionar word\n",
    "                if word in ner:\n",
    "                    ner[word][\"count\"] += 1\n",
    "                    ner[word][\"sentiment\"] += sentiment * sentiment_weight(news[\"newsProbability\"])\n",
    "                    ner[word][\"type\"].add(type)\n",
    "                    if news[\"newsSource\"] not in ner[word][\"source\"]:\n",
    "                        ner[word][\"source\"][news[\"newsSource\"]] = 1\n",
    "                    else:\n",
    "                        ner[word][\"source\"][news[\"newsSource\"]] += 1\n",
    "                    ner[word][\"news\"].add(news[\"linkToArchive\"])\n",
    "                    if news[\"tstamp\"] in ner[word][\"date\"]:\n",
    "                        ner[word][\"date\"][news[\"tstamp\"]] += 1\n",
    "                    else:\n",
    "                        ner[word][\"date\"][news[\"tstamp\"]] = 1\n",
    "                    ner[word][\"weight\"] += count_weight(news[\"newsProbability\"], type)\n",
    "\n",
    "                else: #not seen\n",
    "                    ner[word] = {\"count\": 1,\n",
    "                                 \"sentiment\": sentiment * sentiment_weight(news[\"newsProbability\"]),\n",
    "                                 \"type\": {type},\n",
    "                                 \"source\": {news[\"newsSource\"]: 1},\n",
    "                                 \"news\": {news[\"linkToArchive\"]},\n",
    "                                 \"date\": {news[\"tstamp\"]: 1},\n",
    "                                 \"weight\": count_weight(news[\"newsProbability\"], type)}\n",
    "\n",
    "                    # muitas coisas estao mal classificas como LOC devia ser ORG e assim...\n",
    "    \n",
    "    qtyNews = len(lista)\n",
    "    for word, data in ner.items():\n",
    "        data[\"filter\"] = (data[\"weight\"]/2) / qtyNews # >= 0.1 = \"aparecer em 10% das noticias\"\n",
    "        data[\"sentiment\"] = max(min(data[\"sentiment\"]/data[\"count\"], 1), -1)\n",
    "    filtered_ner = {word: data for word, data in ner.items() if data[\"count\"] > 1}\n",
    "\n",
    "    return filtered_ner\n",
    "\n",
    "df[\"keywords\"] = df[\"news\"].map(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['count', 'sentiment', 'type', 'source', 'news', 'date', 'weight', 'filter'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"keywords\"].iloc[0][\"dinheiro\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when saving the df, parquet adds all the keys to all the diciotnarys, so if a word doents appear for ex. in EDP, it will appear in the dictionary as for example {\"notEDP\": None, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aliases</th>\n",
       "      <th>news</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>companies</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Banco Comercial Português</th>\n",
       "      <td>[Banco Comercial Português, BCP]</td>\n",
       "      <td>[{'ExtractedText': 'DN   13 de Setembro de 200...</td>\n",
       "      <td>{'03 Mar': {'count': 2.0, 'date': {'201503': 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Galp Energia</th>\n",
       "      <td>[Galp Energia, GALP]</td>\n",
       "      <td>[{'ExtractedText': 'RTP Galp reforça posição n...</td>\n",
       "      <td>{'00h00': {'count': 7.0, 'date': {'201004': 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDP</th>\n",
       "      <td>[EDP, Energias de Portugal, Electricidade de P...</td>\n",
       "      <td>[{'ExtractedText': 'DN-Sinteses Negocios 9 de ...</td>\n",
       "      <td>{'00h00': {'count': 4.0, 'date': {'201004': No...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sonae</th>\n",
       "      <td>[Sonae, SON]</td>\n",
       "      <td>[{'ExtractedText': 'DN-Sinteses 5 de Março de ...</td>\n",
       "      <td>{'00h00': {'count': 3.0, 'date': {'201004': No...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mota-Engil</th>\n",
       "      <td>[Mota-Engil, EGL]</td>\n",
       "      <td>[{'ExtractedText': 'RTP Lucro da Mota-Engil so...</td>\n",
       "      <td>{'15h30': {'count': 2.0, 'date': {'201509': 1....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     aliases  \\\n",
       "companies                                                                      \n",
       "Banco Comercial Português                   [Banco Comercial Português, BCP]   \n",
       "Galp Energia                                            [Galp Energia, GALP]   \n",
       "EDP                        [EDP, Energias de Portugal, Electricidade de P...   \n",
       "Sonae                                                           [Sonae, SON]   \n",
       "Mota-Engil                                                 [Mota-Engil, EGL]   \n",
       "\n",
       "                                                                        news  \\\n",
       "companies                                                                      \n",
       "Banco Comercial Português  [{'ExtractedText': 'DN   13 de Setembro de 200...   \n",
       "Galp Energia               [{'ExtractedText': 'RTP Galp reforça posição n...   \n",
       "EDP                        [{'ExtractedText': 'DN-Sinteses Negocios 9 de ...   \n",
       "Sonae                      [{'ExtractedText': 'DN-Sinteses 5 de Março de ...   \n",
       "Mota-Engil                 [{'ExtractedText': 'RTP Lucro da Mota-Engil so...   \n",
       "\n",
       "                                                                    keywords  \n",
       "companies                                                                     \n",
       "Banco Comercial Português  {'03 Mar': {'count': 2.0, 'date': {'201503': 2...  \n",
       "Galp Energia               {'00h00': {'count': 7.0, 'date': {'201004': 1....  \n",
       "EDP                        {'00h00': {'count': 4.0, 'date': {'201004': No...  \n",
       "Sonae                      {'00h00': {'count': 3.0, 'date': {'201004': No...  \n",
       "Mota-Engil                 {'15h30': {'count': 2.0, 'date': {'201509': 1....  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_parquet(\"data05.parquet\")\n",
    "\n",
    "df = pd.read_parquet(\"data05.parquet\")\n",
    "df[\"keywords\"] = df[\"keywords\"].map(lambda dic: {key: dic[key] for key in dic.keys() if dic[key] != None})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentimentos = 0\n",
    "#for key in df.iloc[-1,2].keys():\n",
    "#    sentimentos += df.iloc[-1,2][key][\"sentiment\"]\n",
    "#sentimentos, sentimentos/len(df.iloc[-1,2].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x for x in df[\"keywords\"].iloc[-1].keys() if \"mota\" in x.lower()]\n",
    "#df[\"keywords\"].iloc[-1][\"Mota-Engil África\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcdProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
