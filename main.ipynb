{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objetivo:** é a partir dos nomes e dos aliases de várias empresa, encontrar várias mencoes das mesmas em notícias e tentar ...\n",
    "\n",
    "1. grafo de palavras/pessoas/temas associadas [ver se é positivo / negativo o termo/pessoa]\n",
    "\n",
    "2. relacao entre noticias e stock price\n",
    "\n",
    "3. ...\n",
    "\n",
    "Trabalho tem de ter 3 partes:\n",
    "\n",
    "1. project structure + data acquisition\n",
    "\n",
    "2. exploratory data analysis and visualization\n",
    "\n",
    "3. results & discussion\n",
    "\n",
    "Fonte de Dados: arquivo.pt (https://github.com/arquivo/pwa-technologies/wiki/Arquivo.pt-API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data01.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sites dos quais vamos obter as noticias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news from https://www.kadaza.pt\n",
    "\n",
    "def news(txtFile = 'noticias.txt'):\n",
    "    \"\"\"\n",
    "    grab the news websites from a text file\n",
    "    \"\"\"\n",
    "    with open(txtFile, 'r') as file:\n",
    "        links = file.read().splitlines()\n",
    "    return \",\".join(links)\n",
    "\n",
    "#news()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**como vão ser os api requests / decidir as empresas (PSI20) a analisar / fazer api requests in 3years groups**\n",
    "\n",
    "*1 year to 3 years is long enough to smooth out short-term fluctuations and identify underlying trends. Charts with weekly or monthly intervals over these periods show developments over full economic/market cycles.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_request(search, websites, date):\n",
    "    \"\"\"\n",
    "    search: expression/word (what to look for)\n",
    "    websites: comma separated websites (where to look for)\n",
    "    date: list such as [20030101, 20031231] (when to look for)\n",
    "    -\n",
    "    returns the responde_items from arquivo.pt api\n",
    "    \"\"\"\n",
    "    search = f\"q=%22{search.replace(' ', '%20')}%22\"\n",
    "    websites = f\"&siteSearch={websites}\"\n",
    "    date = f\"&from={date[0]}&to={date[1]}\"    \n",
    "    url = (\n",
    "        f\"https://arquivo.pt/textsearch?{search}{websites}{date}\"\n",
    "        \"&fields=linkToArchive,linkToExtractedText,tstamp\"\n",
    "        \"&maxItems=500&dedupValue=25&dedupField=url&prettyPrint=false&type=html\"\n",
    "        )\n",
    "    json = requests.get(url).json()\n",
    "    data = json[\"response_items\"]\n",
    "    if len(data) == 500:\n",
    "        print(f\"You might have lost some data: {search, date}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datav1(companies):\n",
    "    \"\"\"\n",
    "    this is the function where we choose the companies which will be in study\n",
    "    -\n",
    "    companies should be a dictionary\n",
    "        {\"company1\": [aliases or other names the company is or was known by],\n",
    "        \"company2\": [...]}\n",
    "    -\n",
    "    this data will be saved into a parquet file for future use and with already api requests\n",
    "\n",
    "    also this will do the api requests .... get this better\n",
    "    \"\"\"\n",
    "    # CREATING DF WITH COMPANIES AND THEIR ALIASES\n",
    "    companies_data = {\"companies\": [], \"aliases\": []}\n",
    "    for company in companies.keys():\n",
    "        companies_data[\"companies\"].append(company)\n",
    "        companies_data[\"aliases\"].append(companies[company])\n",
    "    df = pd.DataFrame(companies_data).set_index(\"companies\")\n",
    "\n",
    "    # SITES OF WHERE TO LOOK FOR NEWS\n",
    "    websites = news()\n",
    "\n",
    "    # INITIALIZAING API REQUESTS\n",
    "    # groups of 3 years, from 2000 to 2020\n",
    "    for cluster in range(2000, 2021, 3):\n",
    "        api_cluster = [] #reset api_cluster for each cluster (group of 3 year)\n",
    "        print(f\"Processing cluster: {cluster}\")\n",
    "        print(\"Processing company:\", end=\" \")\n",
    "        # iterate over each company\n",
    "        for company_aliases in df[\"aliases\"]:\n",
    "            api_company = [] #reset api_company for each company\n",
    "            print(f\"{company_aliases[0]}\", end = \"; \")\n",
    "            # iterate over each company's aliases\n",
    "            for alias in company_aliases:\n",
    "                # iterate over each cluter's year\n",
    "                for year in range(cluster, cluster + 3):                        \n",
    "                    api_aliasS1 = api_request(alias, websites, [int(f\"{year}0101\"), int(f\"{year}0630\")])\n",
    "                    api_aliasS2 = api_request(alias, websites, [int(f\"{year}0701\"), int(f\"{year}1231\")])\n",
    "                    api_company += api_aliasS1 + api_aliasS2\n",
    "            # save company data\n",
    "            api_cluster.append(api_company)\n",
    "\n",
    "        # save cluster (group of 3 years) data\n",
    "        df[f\"api.{cluster}\"] = api_cluster\n",
    "        print(f\"{cluster} OK.\")\n",
    "\n",
    "    # save all data\n",
    "    df.to_parquet(\"data01.parquet\")\n",
    "    print(\"Finished.\")\n",
    "    return df\n",
    "\n",
    "companies = {\"Banco Comercial Português\": [\"Banco Comercial Português\", \"BCP\"],\n",
    "             \"Galp Energia\": [\"Galp Energia\", \"GALP\"],\n",
    "             \"EDP\": [\"EDP\", \"Energias de Portugal\", \"Electricidade de Portugal\"],\n",
    "             \"Sonae\": [\"Sonae\", \"SON\"],\n",
    "             \"Mota-Engil\": [\"Mota-Engil\", \"EGL\"]}\n",
    "df01 = datav1(companies)\n",
    "df01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df01.map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data02.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "por ter usado `&dedupValue=25&dedupField=url` e diferentes aliases, há informação repetida\n",
    "\n",
    "**filtrar repetidos e textos que não mencionem nenhum alias**\n",
    "\n",
    "problemas ultrapassados:\n",
    "\n",
    "- API has the following usage limits (250req/min, error 429): `time.sleep(60)`\n",
    "\n",
    "- API error 404 for some urls: return 0 (False) and skip it\n",
    "\n",
    "- extrair o texto demora muito: filtrar e salvar coluna a coluna\n",
    "\n",
    "nota: podia ter feito já online o processamento do texto, mas não queria estar dependente do wifi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracText(linkToExtractedText):\n",
    "    # Infinite loop to handle retry logic in case of 429 Too Many Requests\n",
    "    while True:\n",
    "        response = requests.get(linkToExtractedText)\n",
    "        status_code = response.status_code\n",
    "        \n",
    "        if status_code == 200:\n",
    "            # If the request is successful (200 OK), return the extracted text\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            return soup.get_text()\n",
    "        elif status_code == 429:\n",
    "            # Handle 429 Too Many Requests by reading the Retry-After header\n",
    "            print(\" (...)\", end = \"\")\n",
    "            time.sleep(60)  # Pause execution for the retry period\n",
    "        elif status_code == 404:\n",
    "            return 0\n",
    "        else:\n",
    "            # For any other status codes (e.g., 500, ...), print the status and break the loop\n",
    "            print(f\"Request failed with status code {status_code}. Link was {linkToExtractedText}\")\n",
    "            break\n",
    "\n",
    "# Function to process each column\n",
    "def filterColumn(column, aliases):\n",
    "    \"\"\"aliases in text, repeated text and extract text\"\"\"\n",
    "    global stats\n",
    "    filtered_column = []\n",
    "\n",
    "    for row in aliases.index:\n",
    "        filtered_cell = []\n",
    "        seen_text = set()\n",
    "        print(f\"; {row}\", end = \"\")\n",
    "        for i in column.loc[row]:\n",
    "            \n",
    "            # Extract text from 'linkToExtractedText'\n",
    "            text = extracText(i['linkToExtractedText'])\n",
    "                \n",
    "\n",
    "            # Skip if the text has already been processed\n",
    "            if text in seen_text:\n",
    "                stats[\"duplicate\"] += 1\n",
    "                continue\n",
    "\n",
    "            elif not text: #ERROR 404\n",
    "                stats[\"404\"] += 1\n",
    "                continue\n",
    "            \n",
    "            # Check if any alias is found in the text\n",
    "            elif any(alias.lower() in text.lower() for alias in aliases.loc[row]):\n",
    "                i[\"ExtractedText\"] = text  # Add extracted text to the record\n",
    "                \n",
    "                # Remove unwanted fields\n",
    "                i.pop('linkToExtractedText', None)\n",
    "                \n",
    "                # Append the processed record\n",
    "                filtered_cell.append(i)\n",
    "                \n",
    "                # Mark this text as processed\n",
    "                seen_text.add(text)\n",
    "\n",
    "        filtered_column.append(filtered_cell)\n",
    "                \n",
    "    return filtered_column\n",
    "\n",
    "\n",
    "def processColumns(col_to_proc):\n",
    "    print(f\"Starting: {datetime.now()}\")\n",
    "    try:\n",
    "        # continuar df criada\n",
    "        df = pd.read_parquet(\"data02.parquet\")\n",
    "    except:\n",
    "       # criar df para trabalhar\n",
    "       df = pd.read_parquet(\"data01.parquet\").to_parquet(\"data02.parquet\")\n",
    "       df = pd.read_parquet(\"data02.parquet\")\n",
    "    for column in col_to_proc:\n",
    "        has_link = \"linkToExtractedText\" in df.iloc[-1][column][-1]\n",
    "        has_extracText = \"ExtractedText\" in df.iloc[-1][column][-1]\n",
    "        if not has_link and has_extracText:\n",
    "            print(f\"\\n{column} already done. Skipping.\")\n",
    "        else:\n",
    "            print(f\"\\nProcessing {column}\", end = \": \")\n",
    "            df[column] = filterColumn(df[column], df[\"aliases\"])\n",
    "            df.to_parquet(\"data02.parquet\")\n",
    "    print(f\"\\nEnded: {datetime.now()}.\")\n",
    "\n",
    "stats = {\"404\": 0, \"duplicate\": 0}\n",
    "processColumns([\"api.2000\", \"api.2003\", \"api.2006\", \"api.2009\", \"api.2012\", \"api.2015\", \"api.2018\"])\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(\"data02.parquet\").map(lambda x: len(x)) - pd.read_parquet(\"data01.parquet\").map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(\"data02.parquet\").map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcdProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
