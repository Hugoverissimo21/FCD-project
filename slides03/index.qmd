---
title: "Insights from News and Public Coverage"
subtitle: "InfoMosaic"
author:
  - name: "Hugo Veríssimo"
    affiliation: "124348"
format:
  revealjs: 
    slide-number: true
    chalkboard: 
      buttons: false
    preview-links: auto
    logo: ua.png
    css: mystyle.css
    theme: serif
    transition: slide
echo: true
---

```{r setup, include = FALSE}
# packages
library(dplyr)
library(knitr)
library(xtable)
library(reticulate)
```

```{python, include=FALSE}
import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime
import json
```

## data05.parquet {.justify}

```{=html}
<style>
.dataframe {
  display: block;
  max-width: 100%;
  max-height: 75%; /* vertical scrolling */
  overflow-x: auto;
  overflow-y: auto;
  font-family: "SFMono-Regular", Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
  /*border: 1px solid #ddd; apply only to big dfs */
}

table {
  max-width: 100%;
  border-collapse: collapse;
}

th, td { /* th is about header*/
  padding: 8px 16px;
  border: 1px solid #ddd; /* Border between cells */
  text-align: left;
  vertical-align: middle;
  font-size: 16px;
}

thead th {
  background-color: rgba(128, 128, 128, 0.3);
  font-weight: bold;
}

tbody td:first-child {
  background-color: rgba(128, 128, 128, 0.3);
  font-weight: bold;
}
</style>
```

```{=html}
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th></th>
<th>aliases</th>
<th>news</th>
<th>keywords</th>
</tr>
<tr>
<th>companies</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<th>Banco Comercial Português</th>
<td>[Banco Comercial Português, BCP]</td>
<td>[{'ExtractedText': 'DN &nbsp; 13 de Setembro de 200...</td>
<td>{'03 Mar': {'count': 2.0, 'date': {'201503': 2...</td>
</tr>
<tr>
<th>Galp Energia</th>
<td>[Galp Energia, GALP]</td>
<td>[{'ExtractedText': 'RTP Galp reforça posição n...</td>
<td>{'00h00': {'count': 7.0, 'date': {'201004': 1....</td>
</tr>
<tr>
<th>EDP</th>
<td>[EDP, Energias de Portugal, Electricidade de P...</td>
<td>[{'ExtractedText': 'DN-Sinteses Negocios 9 de ...</td>
<td>{'00h00': {'count': 4.0, 'date': {'201004': No...</td>
</tr>
<tr>
<th>Sonae</th>
<td>[Sonae, SON]</td>
<td>[{'ExtractedText': 'DN-Sinteses 5 de Março de ...</td>
<td>{'00h00': {'count': 3.0, 'date': {'201004': No...</td>
</tr>
<tr>
<th>Mota-Engil</th>
<td>[Mota-Engil, EGL]</td>
<td>[{'ExtractedText': 'RTP Lucro da Mota-Engil so...</td>
<td>{'15h30': {'count': 2.0, 'date': {'201509': 1....</td>
</tr>
</tbody>
</table>
```

```{.python}
pd.read_parquet("data05.parquet")["news"].iloc[0][0].keys()
```
```{python, echo=FALSE}
print("dict_keys(['ExtractedText', 'linkToArchive', 'newsNER', 'newsProbability', 'newsSentiment', 'newsSource', 'tstamp'])" +"\n ")
```

```{.python}
pd.read_parquet("data05.parquet")["keywords"].iloc[0]['03 Mar'].keys()
```
```{python, echo=FALSE}
print("dict_keys(['count', 'date', 'filter', 'news', 'sentiment', 'source', 'type', 'weight'])")
```

## Optimized Data Storage {.justify}

Optimized data organization by saving each cell as a separate JSON file, enhancing loading speed and flexibility.

**news_{company}.json**

```{.python}
json.load(open("news_bcp.json")); .keys()[0] & .values()[0].keys()
```

```{python, echo=FALSE}
print("'https://arquivo.pt/noFrame/replay/20010913052557/http://www.dn.pt/int/13p4x.htm'")
print("dict_keys(['keywords', 'probability', 'sentiment', 'source', 'tstamp'])\n ")
```


**kwrd_{company}.json**

```{.python}
json.load(open("kwrd_bcp.json")); .keys()[:3] & .values()[0].keys()
```

```{python, echo=FALSE}
print("['03 Mar', '10 Nov', '100 Segundos de Ciência']")
print("dict_keys(['count', 'date', 'filter', 'news', 'sentiment', 'source', 'type', 'weight'])")
```

```{=html}
<div class="footer">
  /notebooks/06_DataToJson.ipynb
</div>
```

## News Recommendation System {.justify}

`TfidfVectorizer()` and `cosine_similarity` from `scikit-learn` were used to compute the similarity between news articles.

```{=html}
<figure style="text-align: center;">
    <img src="newsDendrogram.svg" alt="cluster de noticias" style="width: 95%;">
</figure>
```

```{=html}
<div class="footer">
  /tests/newsCluster v04.ipynb
</div>
```

## News Recommendation System {.justify}

```{.python code-line-numbers="1-2|4-14|16-26"}
iteration = 0
ratings = np.array([3.0] * len(news))

def news_recommendation(ratings):
  global iteration
  iteration += 1
  
  # The exponential assigns higher probabilities to larger values 
  weights = np.exp(np.array(ratings)) / weights.sum()
  
  # Select the next suggestion based on the weights
  news_i = np.random.choice(len(news), p = weights)
  
  return news_i

def update_ratings(news_i, user_rating):
    global ratings
    global iteration
    learning_rate = 0.999 * iteration
    
    # Compute similarity to all other texts
    similarity_scores = cosine_similarity(tfidf[news_i], tfidf).flatten()
    
    # Update the ratings for all news
    ratings += (user_rating - ratings) * similarity_scores * learning_rate
    ratings[news_i] = -1000
```

```{=html}
<div class="footer">
  /tests/newsCluster v04.ipynb
</div>
```

## News Recommendation System {.justify}

```{=html}
<figure style="text-align: center; margin-top: 0px;">
    <img src="newsRatings.png" alt="rating de noticias" style="width: 100%;">
</figure>
```

```{=html}
<div class="footer">
  /tests/newsCluster v04.ipynb
</div>
```

# Web Application

## Web Application {.justify}

A web application was developed using `Flask`, integrating various tools for analyzing topics (e.g., companies) based on news articles. It consolidates visualizations and processes created throughout the project into a unified platform, organized into the following sections:

- Explorer

- Topic Map

- Topic Insights

- Word Duel

- Word Cloud

[https://hugover.pythonanywhere.com](https://hugover.pythonanywhere.com)

```{=html}
<div class="footer">
  app.py + /templates/ + /static/
</div>
```

## Explorer {.justify}

```{=html}
<figure style="text-align: center;">
    <img src="flask_main.gif" alt="explorer page" style="width: 90%; border: 2px solid rgb(57, 61, 61);">
</figure>
```

## Topic Map {.justify}

```{=html}
<figure style="text-align: center;">
    <img src="flask_grafo.gif" alt="grafo page" style="width: 90%; border: 2px solid rgb(57, 61, 61);">
</figure>
```

## Topic Insights {.justify}

```{=html}
<figure style="text-align: center;">
    <img src="flask_read.gif" alt="read page" style="width: 90%; border: 2px solid rgb(57, 61, 61);">
</figure>
```

## Word Duel {.justify}

Inspired by [The Higher Lower Game](http://www.higherlowergame.com) and [Noticioso](https://noticioso.pt).

```{=html}
<figure style="text-align: center;">
    <img src="flask_duel.gif" alt="duel page" style="width: 83%; border: 2px solid rgb(57, 61, 61);">
</figure>
```

## Word Cloud {.justify}

```{=html}
<figure style="text-align: center;">
    <img src="flask_wcloud.gif" alt="wcloud page" style="width: 90%; border: 2px solid rgb(57, 61, 61);">
</figure>
```


# Further Improvements

## Generalize the Topic Search {.justify}

As extracting news articles related to a topic, analyzing the keywords within them and assessing the sentiment of each is a time-consuming and computationally expensive process, it is preferable to extract all the news articles from arquivo.pt and analyze them in advance, avoiding redundant operations. This requires changing some of the methodologies used so far.

To achieve this, the CDX Server API from arquivo.pt is used, which returns preserved pages that begin with a specific URL. Over 100 URLs have been selected, including:

```{python, echo=FALSE}
print("""[('https://www.rtp.pt/', 'RTP'),
('https://www.rtp.pt/noticias/', 'RTP'),
('https://www.rtp.pt/noticias/pais/', 'RTP'),
('https://www.rtp.pt/noticias/mundo/', 'RTP'),
('https://www.rtp.pt/noticias/politica/', 'RTP'),
('https://www.rtp.pt/noticias/economia/', 'RTP'), ...]
""")
```


```{=html}
<div class="footer">
  https://github.com/Hugoverissimo21/InfoMosaic/blob/main/data01%20newsUrls.ipynb
</div>
```

## Processing CDX API Results {.justify}

To process the 3 056 418 results from the CDX API, tools and methods such as Apache Spark, Bloom Filters, Logistic Regression and Probabilistic Counters are being used. The processing approach for extracting sentiment and keywords from each news result is very similar to the one used so far.

```{python, echo=FALSE}
print("""
root
 |-- timestamp: integer (nullable = true)
 |-- source: string (nullable = true)
 |-- archive: string (nullable = true)
 |-- id: integer (nullable = true)
 |-- probability: float (nullable = true)
 |-- keywords: map (nullable = true)
 |    |-- key: string
 |    |-- value: integer (valueContainsNull = true)
 |-- sentiment: float (nullable = true)
 """)
```

```{=html}
<div class="footer">
  https://github.com/Hugoverissimo21/InfoMosaic/blob/main/data02.py
</div>
```

## News Detection {.justify}

A new machine learning model was developed, specifically a logistic regression, to distinguish between news and non-news articles. TF-IDF was used for feature extraction, and the hyperparameters were optimized to maximize the recall of the "news" class.

$\ $

```{=html}
<figure style="text-align: center;">
    <img src="MLmetrics_LRnews.png" alt="metrics class news ML" style="width: 65%;">
    <figcaption style="font-size: 0.45em; margin-top: -22px;">Evaluation metrics for logistic regression classification of the 'news' class.</figcaption>
</figure>
```


```{=html}
<div class="footer">
  https://github.com/Hugoverissimo21/InfoMosaic/tree/main/assets
</div>
```

## Remaining Tasks {.justify}

- Process the remaining results from CDX API.

- Store the processed data in a database, such as MongoDB.

- Design and implement an algorithm to efficiently identify news articles relevant to the user's search topic and convert the data into the required input format for the web application.

- Implement the solution into the web application, ensuring it is optimized for computational performance.

- Improve the user interface.

