{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from joblib import load\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANIES = [\"BCP\", \"Banco Comercial Português\"]\n",
    "companies = COMPANIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINKS = {\"www.publico.pt\": \"Público\", #from https://www.kadaza.pt\n",
    "         \"publico.pt\": \"Público\",\n",
    "         \"www.dn.pt\": \"Diário de Notícias\",\n",
    "         \"www.rtp.pt\": \"RTP\",\n",
    "         \"rpt.pt\": \"RTP\",\n",
    "         \"www.cmjornal.pt\": \"Correio da Manhã\",\n",
    "         \"www.iol.pt\": \"IOL\",\n",
    "         \"www.tvi24.iol.pt\": \"TVI\",\n",
    "         \"tvi24.iol.pt\": \"TVI\",\n",
    "         \"noticias.sapo.pt\": \"Sapo\",\n",
    "         \"observador.pt\": \"Observador\",\n",
    "         \"expresso.pt\": \"Expresso\",\n",
    "         \"www.expresso.pt\": \"Expresso\",\n",
    "         \"sol.sapo.pt\": \"SOL\",\n",
    "         \"www.jornaldenegocios.pt\": \"Jornal de Negócios\",\n",
    "         \"www.jn.pt\": \"Jornal de Notícias\",\n",
    "         \"jn.pt\": \"Jornal de Notícias\",\n",
    "         \"ionline.pt\": \"Jornal i\",\n",
    "         \"sicnoticias.pt\": \"SIC\",\n",
    "         \"www.sicnoticias.pt\": \"SIC\",\n",
    "         \"www.lux.iol.pt\": \"Lux\",\n",
    "         \"www.ionline.pt\": \"Jornal i\",\n",
    "         \"news.google.pt\": \"Google\",\n",
    "         \"www.dinheirovivo.pt\": \"Dinheiro Vivo\",\n",
    "         \"www.aeiou.pt\": \"AEIOU\",\n",
    "         \"aeiou.pt\": \"AEIOU\",\n",
    "         \"www.tsf.pt\": \"TSF\",\n",
    "         \"tsf.pt\": \"TSF\",\n",
    "         \"www.sabado.pt\": \"Sábado\",\n",
    "         \"economico.sapo.pt\": \"Jornal Económico\",\n",
    "         \"cnnportugal.iol.pt\": \"CNN Portugal\"}\n",
    "\n",
    "news_sources = LINKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_request(search, websites, date, dedup):\n",
    "    \"\"\"\n",
    "    Makes a request to the arquivo.pt API to search for a specific term within specified websites and date range.\n",
    "\n",
    "    Parameters:\n",
    "    search (str): The expression or word to look for.\n",
    "    websites (str): Comma-separated websites where the search should be performed.\n",
    "    date (list): A list containing two dates in the format [YYYYMMDD, YYYYMMDD] representing the start and end dates for the search.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of response items from the arquivo.pt API.\n",
    "\n",
    "    Notes:\n",
    "    - The function constructs a URL for the API request based on the provided parameters.\n",
    "    - If the number of response items is 500, a message is printed indicating that some data might have been lost due to the limit.\n",
    "    \"\"\"\n",
    "    search = f\"q=%22{search.replace(' ', '%20')}%22\"\n",
    "    websites = f\"&siteSearch={websites}\"\n",
    "    date = f\"&from={date[0]}&to={date[1]}\"    \n",
    "    url = (\n",
    "        f\"https://arquivo.pt/textsearch?{search}{websites}{date}\"\n",
    "        \"&fields=linkToExtractedText,tstamp,linkToNoFrame\"\n",
    "        f\"&maxItems=500&dedupValue={dedup}&dedupField=url&prettyPrint=false&type=html\"\n",
    "        )\n",
    "    json = requests.get(url).json()\n",
    "    data = json[\"response_items\"]\n",
    "    if len(data) == 500:\n",
    "        print(f\"You might have lost some data: {search, date}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_request_bulk(companies, news_sources=news_sources):\n",
    "    \"\"\"\n",
    "    Makes bulk API requests for a list of companies over a range of years and returns the results.\n",
    "\n",
    "    Args:\n",
    "        companies (list): A list of company aliases to request data for.\n",
    "        news_sources (dict): A dictionary of news sources with their corresponding keys.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where each key is a link to a no-frame version of the article, and each value is a dictionary containing:\n",
    "            - \"tstamp\" (int): The timestamp of the article in the format YYYYMM.\n",
    "            - \"linkToExtractedText\" (str): The link to the extracted text of the article.\n",
    "            - \"source\" (str): The source of the article.\n",
    "    \"\"\"\n",
    "    websites = \",\".join(list(news_sources.keys()))\n",
    "    api_answer = {}\n",
    "    for alias in companies:\n",
    "        for year in range(2000, 2021):\n",
    "            if year < 2010:\n",
    "                dedup = 25\n",
    "            else:\n",
    "                dedup = 2\n",
    "            api_aliasS1 = api_request(alias, websites, [int(f\"{year}0101\"), int(f\"{year}0630\")], dedup)\n",
    "            api_aliasS2 = api_request(alias, websites, [int(f\"{year}0701\"), int(f\"{year}1231\")], dedup)\n",
    "            api_alias = api_aliasS1 + api_aliasS2\n",
    "            for item in api_alias:\n",
    "                source = \"unknown\"\n",
    "                for wbsite in news_sources.keys():\n",
    "                    if wbsite in item[\"linkToNoFrame\"]:\n",
    "                        source = news_sources[wbsite]\n",
    "                        break\n",
    "                api_answer[item[\"linkToNoFrame\"]] = {\"tstamp\": int(str(item[\"tstamp\"])[:6]),\n",
    "                                                      \"linkToExtractedText\": item[\"linkToExtractedText\"],\n",
    "                                                      \"source\": source}\n",
    "    return api_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracText(linkToExtractedText):\n",
    "    \"\"\"\n",
    "    Extracts text content from a given URL.\n",
    "\n",
    "    Args:\n",
    "        linkToExtractedText (str): The URL from which to extract text.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted text content if the request is successful.\n",
    "        str: \"1min\" if the request is rate-limited (status code 429).\n",
    "        int: 404 if the requested resource is not found (status code 404).\n",
    "        int: 404 if the request fails with any other status code.\n",
    "\n",
    "    Raises:\n",
    "        requests.exceptions.RequestException: If there is an issue with the HTTP request.\n",
    "    \"\"\"\n",
    "    response = requests.get(linkToExtractedText)\n",
    "    status_code = response.status_code\n",
    "    if status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        return soup.get_text()\n",
    "    elif status_code == 429:\n",
    "        return \"1min\"\n",
    "    elif status_code == 404:\n",
    "        return 404\n",
    "    else:\n",
    "        print(f\"Request failed with status code {status_code}. Link was {linkToExtractedText}\")\n",
    "        return 404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES for ML model for news filtering\n",
    "# [['IstALIAS', 'propAN', 'txtSZ', 'countALI', 'countDTS', 'countHOUR', 'countCAPS']]\n",
    "#\n",
    "#This module provides feature extraction functions for a machine learning model used for news filtering.\n",
    "#\n",
    "#Functions:\n",
    "#    IstALIAS(text, aliases):\n",
    "#        Determines the position of the first alias in the text and returns the number of words before it.\n",
    "#    \n",
    "#    propAN(text, aliases):\n",
    "#        Calculates the proportion of alphanumeric characters in the text.\n",
    "#    \n",
    "#    txtSZ(text, aliases):\n",
    "#        Returns the size (length) of the text.\n",
    "#    \n",
    "#    countALI(text, aliases):\n",
    "#        Counts the number of times any alias appears in the text.\n",
    "#    \n",
    "#    countDTS(text, aliases):\n",
    "#        Counts the number of date occurrences in the text.\n",
    "#    \n",
    "#    countHOUR(text, aliases):\n",
    "#        Counts the number of time occurrences (in hh:mm format) in the text.\n",
    "#    \n",
    "#    countCAPS(text, aliases):\n",
    "#        Counts the number of uppercase words in the text.\n",
    "\n",
    "def IstALIAS(text, aliases):\n",
    "    \"\"\"where does the first alias appear, title?\"\"\"\n",
    "    indexs = []\n",
    "    for alias in aliases:\n",
    "        index = text.lower().find(alias.lower())\n",
    "        if index != -1:\n",
    "            indexs.append(index)\n",
    "    try:\n",
    "        a = text[:min(indexs)].count(' ')\n",
    "    except:\n",
    "        a = 10000000000000000000\n",
    "    return a\n",
    "\n",
    "def propAN(text, aliases):\n",
    "    \"\"\"proportion of alphanumeric chars in the text\"\"\"\n",
    "    alphanumeric_chars = sum(char.isalnum() for char in text)\n",
    "    proportion = alphanumeric_chars / len(text)\n",
    "    return proportion\n",
    "\n",
    "def txtSZ(text, aliases):\n",
    "    \"\"\"text size\"\"\"\n",
    "    return len(text)\n",
    "\n",
    "def countALI(text, aliases):\n",
    "    \"\"\"count how many aliases appear in the text\"\"\"\n",
    "    alias_count = {expression: 0 for expression in aliases}\n",
    "    for alias in aliases:\n",
    "        # Use re.escape to handle any special characters in the expression\n",
    "        pattern = re.escape(alias.lower())\n",
    "        matches = re.findall(alias, text.lower())\n",
    "        alias_count[alias] = len(matches)\n",
    "    return sum(alias_count.values())\n",
    "\n",
    "def countDTS(text, aliases):\n",
    "    \"\"\"count how many dates appear in the text\"\"\"\n",
    "    date_pattern = r'\\b(\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}|\\d{4}[-/]\\d{1,2}[-/]\\d{1,2})\\b'\n",
    "    # 10/11/2024', '10/10/2024', '12-25-1990', '2024-11-05', '01/10/2024'\n",
    "    dates = re.findall(date_pattern, text)\n",
    "    date_count = len(dates)\n",
    "    return date_count\n",
    "\n",
    "def countHOUR(text, aliases):\n",
    "    \"\"\"count how many hours (ex.: hh:mm) appear in the text\"\"\"\n",
    "    time_pattern = r'\\b([01]?[0-9]|2[0-3]):[0-5][0-9]\\b'   \n",
    "    occurrences = re.findall(time_pattern, text)\n",
    "    return len(occurrences)\n",
    "\n",
    "def countCAPS(text, aliases):\n",
    "    \"\"\"count how many WORDS are upper\"\"\"\n",
    "    words = text.split()\n",
    "    uppercase_word_count = sum(1 for word in words if word.isupper())\n",
    "    return uppercase_word_count\n",
    "\n",
    "# Load the saved model\n",
    "clf = load('dtree01.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case its needed to filter sentences which only contain  keywords\n",
    "def filter_sentences_by_keywords(text, aliases):\n",
    "    \"\"\"\n",
    "    Filters sentences from the given text that contain any of the specified keywords (aliases).\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be filtered.\n",
    "        aliases (list of str): A list of keywords to filter sentences by.\n",
    "\n",
    "    Returns:\n",
    "        str: A single string containing the filtered sentences joined together.\n",
    "    \"\"\"\n",
    "    # Split the text by punctuation and also by multiple spaces or newlines\n",
    "    sentences = re.split(r'(?<=[.!?]) +|\\s{2,}|\\n+', text)\n",
    "    # Filter sentences that contain any of the aliases\n",
    "    filtered_sentences = [sentence for sentence in sentences if any(keyword.lower() in sentence.lower() for keyword in aliases)]\n",
    "    # Join the filtered sentences back into a single string\n",
    "    filtered_text = ' '.join(filtered_sentences)\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearDuplicates(text, lista, threshold=90):\n",
    "    \"\"\"\n",
    "    Check if a given text is a near duplicate of any text in a list based on a similarity threshold.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to check for near duplicates.\n",
    "        lista (list of str): The list of texts to compare against.\n",
    "        threshold (int, optional): The similarity threshold (default is 90).\n",
    "\n",
    "    Returns:\n",
    "        bool: True if a near duplicate is found, False otherwise.\n",
    "    \"\"\"\n",
    "    for txt in lista:\n",
    "        if fuzz.ratio(txt, text) > threshold:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOMETHING IS WRONG WITH THIS NEXT CODE, DELETING TO MANY NEWS IDK WHY !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterNews(news_json, companies = companies):\n",
    "    features = ['IstALIAS', 'propAN', 'txtSZ', 'countALI', 'countDTS', 'countHOUR', 'countCAPS']\n",
    "    news_index = -1\n",
    "    news_list = list(news_json.keys())\n",
    "    delete_news = []\n",
    "    processed_news = []\n",
    "    df_features = {}\n",
    "    while news_index < len(news_list)-1:\n",
    "        news_index += 1\n",
    "        news = news_list[news_index]\n",
    "        text = extracText(news_json[news]['linkToExtractedText'])\n",
    "        if text == 404:\n",
    "            continue\n",
    "        elif text == \"1min\":\n",
    "            news_index -= 1\n",
    "            # Process the news that have been extracted so far (could be a function)\n",
    "            curr_news = news_index\n",
    "            for ExtractedText in reversed(processed_news):\n",
    "                for feature in features:\n",
    "                    df_features[feature] = [globals()[feature](ExtractedText, companies)]\n",
    "                probability = clf.predict_proba(pd.DataFrame(df_features))[0, 1]\n",
    "                if probability < 0.1: #.4\n",
    "                    delete_news.append(news_list[curr_news])\n",
    "                    pass\n",
    "                elif probability <= 0.6:\n",
    "                    news_json[news_list[curr_news]][\"probability\"] = round(probability, 3)\n",
    "                    ExtractedText = filter_sentences_by_keywords(ExtractedText, companies)\n",
    "                else:\n",
    "                    news_json[news_list[curr_news]][\"probability\"] = round(probability, 3)\n",
    "                curr_news -= 1\n",
    "            processed_news = []\n",
    "\n",
    "        else:\n",
    "            if text in processed_news:\n",
    "                processed_news.append(\" \")\n",
    "            elif nearDuplicates(text, processed_news):\n",
    "                processed_news.append(\" \")\n",
    "            else:\n",
    "                processed_news.append(news)\n",
    "        \n",
    "    # If the last news is reached, process the remaining news (could be a function)\n",
    "    curr_news = news_index\n",
    "    for ExtractedText in reversed(processed_news):\n",
    "        for feature in features:\n",
    "            df_features[feature] = [globals()[feature](ExtractedText, companies)]\n",
    "        probability = clf.predict_proba(pd.DataFrame(df_features))[0, 1]\n",
    "        if probability < 0.1:\n",
    "            delete_news.append(news_list[curr_news])\n",
    "            pass\n",
    "        elif probability <= 0.6:\n",
    "            news_json[news_list[curr_news]][\"probability\"] = round(probability, 3)\n",
    "            ExtractedText = filter_sentences_by_keywords(ExtractedText, companies)\n",
    "        else:\n",
    "            news_json[news_list[curr_news]][\"probability\"] = round(probability, 3)\n",
    "        curr_news -= 1\n",
    "\n",
    "    print(f\"News to delete: {len(delete_news)}\")\n",
    "    for news_to_delete in delete_news:\n",
    "        news_json.pop(news_to_delete, None)\n",
    "                \n",
    "    return news_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1459\n",
      "141\n"
     ]
    }
   ],
   "source": [
    "news_json = api_request_bulk(companies)\n",
    "print(len(news_json))\n",
    "news_json1 = filterNews(news_json)\n",
    "print(len(news_json1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6m 53s | 6m 22s, 141 | 1009 141 | original: 807"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISSING NEW AND SENT, maybe give bool if sentiment or not, because its only used in graph maybe not always necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcdProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
